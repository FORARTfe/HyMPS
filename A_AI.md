# [![AUDIO](https://flat.badgen.net/badge/HyMPS/AUDIO/green?scale=1.8)](https://github.com/forart/HyMPS#- "AUDIO resources") [![AI-based projects](https://flat.badgen.net/badge/HyMPS/AI-based%20projects/blue?scale=1.8&label=)](https://github.com/forart/HyMPS#ai-based "AI-based") <img align="right" alt="stable" src="https://user-images.githubusercontent.com/171307/210727719-14b940a2-d1dc-4991-b6a4-7add74463ce8.png" width="5%" />

### [Voices](#voices-) - [Guitars](#guitars-) - [Bass](#bass-) - [Drums](#drums-) - [MIDI](#midi-) - [Mixing](#mixing-) - [Fingerprinting](#fingerprinting-) - [Source Separation](#source-separation-) - [Watermarking](#watermarking-) - [Codecs](#codecs-) - [Misc](#misc-)

$\color{orange}\textsf{\Large\&#x24D8;\kern{0.1cm}\small {SORTING: Language (a>z) > License (openness) > Name / URL (a>z)}}$ 

### Voice [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Applio](https://github.com/IAHispano/Applio#readme)|Ultimate voice cloning tool, meticulously optimized for unrivaled power, modularity, and user-friendly experience|[![](https://img.shields.io/github/languages/top/IAHispano/Applio?color=pink&style=flat-square)](https://github.com/IAHispano/Applio/graphs/contributors)|[![](https://flat.badgen.net/badge/license/OTHER/blue?label=)](https://github.com/IAHispano/Applio/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/IAHispano/Applio?label=)](https://github.com/IAHispano/Applio/graphs/code-frequency)|
|[Real-Time Voice Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning#readme)|Clone a voice in 5 seconds to generate arbitrary speech in real-time|[![](https://img.shields.io/github/languages/top/CorentinJ/Real-Time-Voice-Cloning?color=pink&style=flat-square)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/graphs/contributors)|[![](https://flat.badgen.net/github/license/CorentinJ/Real-Time-Voice-Cloning?label=)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/CorentinJ/Real-Time-Voice-Cloning?label=)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/graphs/code-frequency)|

### Guitars [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[SmartAmpPro](https://github.com/GuitarML/SmartAmpPro#readme)|Guitar plugin made with JUCE that uses neural network models to emulate real world hardware|[![](https://img.shields.io/github/languages/top/GuitarML/SmartAmpPro?color=pink&style=flat-square)](https://github.com/GuitarML/SmartAmpPro/graphs/contributors)|[![](https://flat.badgen.net/github/license/GuitarML/SmartAmpPro?label=)](https://github.com/GuitarML/SmartAmpPro/blob/main/LICENSE.txt)|[![](https://flat.badgen.net/github/last-commit/GuitarML/SmartAmpPro?label=)](https://github.com/GuitarML/SmartAmpPro/graphs/code-frequency)|
|[SmartGuitarAmp](https://github.com/GuitarML/SmartGuitarAmp#readme)|Guitar plugin made with JUCE that uses neural network models to emulate real world hardware|[![](https://img.shields.io/github/languages/top/GuitarML/SmartGuitarAmp?color=pink&style=flat-square)](https://github.com/GuitarML/SmartGuitarAmp/graphs/contributors)|[![](https://flat.badgen.net/github/license/GuitarML/SmartGuitarAmp?label=)](https://github.com/GuitarML/SmartGuitarAmp/blob/main/LICENSE.txt)|[![](https://flat.badgen.net/github/last-commit/GuitarML/SmartGuitarAmp?label=)](https://github.com/GuitarML/SmartGuitarAmp/graphs/code-frequency)|
|[GuitarAmp](https://github.com/apohl79/GuitarAmp#readme)|Guitar plugin using neural networks to capture real amps and pedals|[![](https://img.shields.io/github/languages/top/apohl79/GuitarAmp?color=pink&style=flat-square)](https://github.com/apohl79/GuitarAmp/graphs/contributors)|[![](https://flat.badgen.net/badge/license/OWN/blue?label=)](https://www.cockos.com/wdl/)|[![](https://flat.badgen.net/github/last-commit/apohl79/GuitarAmp?label=)](https://github.com/apohl79/GuitarAmp/graphs/code-frequency)|
|[Automated-Guitar Amplifier Modelling](https://github.com/Alec-Wright/Automated-GuitarAmpModelling#readme)|Neural network training scripts and trained models of guitar amplifiers and distortion pedals|[![](https://img.shields.io/github/languages/top/Alec-Wright/Automated-GuitarAmpModelling?color=pink&style=flat-square)](https://github.com/Alec-Wright/Automated-GuitarAmpModelling/graphs/contributors)|[![](https://flat.badgen.net/github/license/Alec-Wright/Automated-GuitarAmpModelling?label=)](https://github.com/Alec-Wright/Automated-GuitarAmpModelling/blob/main/LICENSE.md)|[![](https://flat.badgen.net/github/last-commit/Alec-Wright/Automated-GuitarAmpModelling?label=)](https://github.com/Alec-Wright/Automated-GuitarAmpModelling/graphs/code-frequency)|
|[GuitarLSTM](https://github.com/GuitarML/GuitarLSTM#readme)|Deep learning models for guitar amp/pedal emulation using LSTM with Keras|[![](https://img.shields.io/github/languages/top/GuitarML/GuitarLSTM?color=pink&style=flat-square)](https://github.com/GuitarML/GuitarLSTM/graphs/contributors)|[![](https://flat.badgen.net/github/license/GuitarML/GuitarLSTM?label=)](https://github.com/GuitarML/GuitarLSTM/blob/main/LICENSE.txt)|[![](https://flat.badgen.net/github/last-commit/GuitarML/GuitarLSTM?label=)](https://github.com/GuitarML/GuitarLSTM/graphs/code-frequency)|
|[NAM: neural amp modeler](https://github.com/sdatkinson/neural-amp-modeler#readme)|Neural network emulator for guitar amplifiers|[![](https://img.shields.io/github/languages/top/sdatkinson/neural-amp-modeler?color=pink&style=flat-square)](https://github.com/sdatkinson/neural-amp-modeler/graphs/contributors)|[![](https://flat.badgen.net/github/license/sdatkinson/neural-amp-modeler?label=)](https://github.com/sdatkinson/neural-amp-modeler/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/sdatkinson/neural-amp-modeler?label=)](https://github.com/sdatkinson/neural-amp-modeler/graphs/code-frequency)|
|[CNN Distortion](https://github.com/mganger/cnn-distortion#readme)|Combine deep learning and DSP|[![](https://img.shields.io/github/languages/top/mganger/cnn-distortion?color=pink&style=flat-square)](https://github.com/mganger/cnn-distortion/graphs/contributors)|[![](https://flat.badgen.net/github/license/mganger/cnn-distortion?label=)](https://github.com/mganger/cnn-distortion/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/mganger/cnn-distortion?label=)](https://github.com/mganger/cnn-distortion/graphs/code-frequency)|
|[Deep Guitar Amplifier](https://github.com/salvatorefara/deepGuitarAmp#readme)|A little project to practice Tensorflow/Keras where I use deep learning for black-box modelling of a guitar amplifier|[![](https://img.shields.io/github/languages/top/salvatorefara/deepGuitarAmp?color=pink&style=flat-square)](https://github.com/salvatorefara/deepGuitarAmp/graphs/contributors)|[![](https://flat.badgen.net/github/license/salvatorefara/deepGuitarAmp?label=)](https://github.com/salvatorefara/deepGuitarAmp/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/salvatorefara/deepGuitarAmp?label=)](https://github.com/salvatorefara/deepGuitarAmp/graphs/code-frequency)|

### Bass [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Walking Bass Transcription](https://github.com/jakobabesser/walking_bass_transcription_dnn#readme)|Algorithm for walking bass transcription in jazz ensemble recordings using Deep Neural Networks (DNN)|[![](https://img.shields.io/github/languages/top/jakobabesser/walking_bass_transcription_dnn?color=pink&style=flat-square)](https://github.com/jakobabesser/walking_bass_transcription_dnn/graphs/contributors)|[![](https://flat.badgen.net/github/license/jakobabesser/walking_bass_transcription_dnn?label=)](https://github.com/jakobabesser/walking_bass_transcription_dnn/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/jakobabesser/walking_bass_transcription_dnn?label=)](https://github.com/jakobabesser/walking_bass_transcription_dnn/graphs/code-frequency)|
|[BassUNet](https://github.com/jakobabesser/bassunet#readme)|Algorithm for bass transcription (joint frame-level pitch and voicing estimation) using U-Net Fully Convolutional Networks|[![](https://img.shields.io/github/languages/top/jakobabesser/bassunet?color=pink&style=flat-square)](https://github.com/jakobabesser/bassunet/graphs/contributors)|[![](https://flat.badgen.net/github/license/jakobabesser/bassunet?label=)](https://github.com/jakobabesser/bassunet/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/jakobabesser/bassunet?label=)](https://github.com/jakobabesser/bassunet/graphs/code-frequency)|
|[bassTranscriber](https://github.com/NicholasBlaskey/bassTranscriber#readme)|Automatically transcribing bass lines using neural networks|[![](https://img.shields.io/github/languages/top/NicholasBlaskey/bassTranscriber?color=pink&style=flat-square)](https://github.com/NicholasBlaskey/bassTranscriber/graphs/contributors)|[![](https://flat.badgen.net/github/license/NicholasBlaskey/bassTranscriber?label=)](https://github.com/NicholasBlaskey/bassTranscriber/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/NicholasBlaskey/bassTranscriber?label=)](https://github.com/NicholasBlaskey/bassTranscriber/graphs/code-frequency)|
|[Beatle-Basslines](https://github.com/jmineroff/Beatle-Basslines#readme)|Deep Learning model for creation of an instrument track in a performer's style from other tracks in a MIDI file|[![](https://img.shields.io/github/languages/top/jmineroff/Beatle-Basslines?color=pink&style=flat-square)](https://github.com/jmineroff/Beatle-Basslines/graphs/contributors)|[![](https://flat.badgen.net/github/license/jmineroff/Beatle-Basslines?label=)](https://github.com/jmineroff/Beatle-Basslines/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/jmineroff/Beatle-Basslines/master?label=)](https://github.com/jmineroff/Beatle-Basslines/graphs/code-frequency)|

### Drums [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Automatic Drum Transcription](https://github.com/underson14/automatic-drum-transcription#readme)|Automatic drum transcription using neural nets|[![](https://img.shields.io/github/languages/top/underson14/automatic-drum-transcription?color=pink&style=flat-square)](https://github.com/underson14/automatic-drum-transcription/graphs/contributors)|[![](https://flat.badgen.net/github/license/underson14/automatic-drum-transcription?label=)](https://github.com/underson14/automatic-drum-transcription/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/underson14/automatic-drum-transcription?label=)](https://github.com/underson14/automatic-drum-transcription/graphs/code-frequency)|
|[Mix-Wave-U-Net](https://github.com/f90/Mix-Wave-U-Net#readme)|Implementation of the [Mix-Wave-U-Net](https://www.aes.org/e-lib/browse.cfm?elib=21023) for automatic mixing of drums|[![](https://img.shields.io/github/languages/top/f90/Mix-Wave-U-Net?color=pink&style=flat-square)](https://github.com/f90/Mix-Wave-U-Net/graphs/contributors)|[![](https://flat.badgen.net/github/license/f90/Mix-Wave-U-Net?label=)](https://github.com/f90/Mix-Wave-U-Net/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/f90/Mix-Wave-U-Net?label=)](https://github.com/f90/Mix-Wave-U-Net/graphs/code-frequency)|
|[NeuralDrummer](https://github.com/bdshrk/neuraldrummer#readme)|A neural network for generating drum tracks for songs|[![](https://img.shields.io/github/languages/top/bdshrk/neuraldrummer?color=pink&style=flat-square)](https://github.com/bdshrk/neuraldrummer/graphs/contributors)|[![](https://flat.badgen.net/github/license/bdshrk/neuraldrummer?label=)](https://github.com/bdshrk/neuraldrummer/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/bdshrk/neuraldrummer?label=)](https://github.com/bdshrk/neuraldrummer/graphs/code-frequency)|
|[Neural-Networks-for-Drum-Music-Generation](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation#readme)|Generating realistic drum music using LSTM neural networks trained on rock-style MIDI drum performances|[![](https://img.shields.io/github/languages/top/pareshraut/Neural-Networks-for-Drum-Music-Generation?color=pink&style=flat-square)](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation/graphs/contributors)|[![](https://flat.badgen.net/github/license/pareshraut/Neural-Networks-for-Drum-Music-Generation?label=)](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/pareshraut/Neural-Networks-for-Drum-Music-Generation?label=)](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation/graphs/code-frequency)|
|[Automatic drums transcription using neural networks](https://github.com/pagrumiaux/drums_transcription#readme)|Internship's code for automatic drums transcription with neural networks|[![](https://img.shields.io/github/languages/top/pagrumiaux/drums_transcription?color=pink&style=flat-square)](https://github.com/pagrumiaux/drums_transcription/graphs/contributors)|[![](https://flat.badgen.net/github/license/pagrumiaux/drums_transcription?label=)](https://github.com/pagrumiaux/drums_transcription/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/pagrumiaux/drums_transcription?label=)](https://github.com/pagrumiaux/drums_transcription/graphs/code-frequency)|
|[drumsep](https://github.com/morehovschi/drumsep#readme)|A Convolutional Neural Network for drum signal separation from full mixes|[![](https://img.shields.io/github/languages/top/morehovschi/drumsep?color=pink&style=flat-square)](https://github.com/morehovschi/drumsep/graphs/contributors)|[![](https://flat.badgen.net/github/license/morehovschi/drumsep?label=)](https://github.com/morehovschi/drumsep/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/morehovschi/drumsep?label=)](https://github.com/morehovschi/drumsep/graphs/code-frequency)|
|[Generative Adversarial Networks - Drum Pattern Generation](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator#readme)|It generates drum patterns similar to those by [Maciej Kowalski](https://www.metal-archives.com/artists/Maciej_Kowalski/10225/)|[![](https://img.shields.io/github/languages/top/omerkolcak/GANs-Drum-Pattern-Generator?color=pink&style=flat-square)](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator/graphs/contributors)|[![](https://flat.badgen.net/github/license/omerkolcak/GANs-Drum-Pattern-Generator?label=)](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/omerkolcak/GANs-Drum-Pattern-Generator?label=)](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator/graphs/code-frequency)|

### MIDI [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[NeuralNote](https://github.com/DamRsn/NeuralNote#readme)|Audio Plugin for Audio to MIDI transcription using deep learning|[![](https://img.shields.io/github/languages/top/DamRsn/NeuralNote?color=pink&style=flat-square)](https://github.com/DamRsn/NeuralNote/graphs/contributors)|[![](https://flat.badgen.net/github/license/DamRsn/NeuralNote?label=)](https://github.com/DamRsn/NeuralNote/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/DamRsn/NeuralNote/master?label=)](https://github.com/DamRsn/NeuralNote/graphs/code-frequency)|
|[Notochord](https://github.com/Intelligent-Instruments-Lab/notochord#readme)|A real-time neural network model for MIDI performances|[![](https://img.shields.io/github/languages/top/Intelligent-Instruments-Lab/notochord?color=pink&style=flat-square)](https://github.com/Intelligent-Instruments-Lab/notochord/graphs/contributors)|[![](https://flat.badgen.net/github/license/Intelligent-Instruments-Lab/notochord?label=)](https://github.com/Intelligent-Instruments-Lab/notochord/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/Intelligent-Instruments-Lab/notochord?label=)](https://github.com/Intelligent-Instruments-Lab/notochord/graphs/code-frequency)|
|[RoboDrummer](https://github.com/lincolt/RoboDrummer#readme)|Midi drums generator based on deep neural network|[![](https://img.shields.io/github/languages/top/lincolt/RoboDrummer?color=pink&style=flat-square)](https://github.com/lincolt/RoboDrummer/graphs/contributors)|[![](https://flat.badgen.net/github/license/lincolt/RoboDrummer?label=)](https://github.com/lincolt/RoboDrummer/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/lincolt/RoboDrummer?label=)](https://github.com/lincolt/RoboDrummer/graphs/code-frequency)|

### Mixing [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Evolving artificial neural networks for cross-adaptive audio effects](http://iver56.github.io/cross-adaptive-audio/)|Analysis of various features of the audio signal is used to adaptively control parameters of audio processing of the same signal|[![](https://img.shields.io/github/languages/top/iver56/cross-adaptive-audio?color=pink&style=flat-square)](https://github.com/iver56/cross-adaptive-audio/graphs/contributors)|[![](https://flat.badgen.net/github/license/iver56/cross-adaptive-audio?label=)](https://github.com/iver56/cross-adaptive-audio/blob/master/LICENCE)|[![](https://flat.badgen.net/github/last-commit/iver56/cross-adaptive-audio?label=)](https://github.com/iver56/cross-adaptive-audio/graphs/code-frequency)|
|[automix-toolkit](https://github.com/csteinmetz1/automix-toolkit#readme)|Models and datasets for training deep learning automatic mixing models|[![](https://img.shields.io/github/languages/top/csteinmetz1/automix-toolkit?color=pink&style=flat-square)](https://github.com/csteinmetz1/automix-toolkit/graphs/contributors)|[![](https://flat.badgen.net/github/license/csteinmetz1/automix-toolkit?label=)](https://github.com/csteinmetz1/automix-toolkit/blob/main/LICENSE)|[![](https://flat.badgen.net/github/last-commit/csteinmetz1/automix-toolkit/main?label=)](https://github.com/csteinmetz1/automix-toolkit/graphs/code-frequency)|
|[DJtransGAN](https://github.com/ChenPaulYu/DJtransGAN#djtransgan-automatic-dj-transitions-with-differentiable-audio-effects-and-generative-adversarial-networks)|Automatic DJ Transitions with Differentiable Audio Effects and Generative Adversarial Networks|[![](https://img.shields.io/github/languages/top/ChenPaulYu/DJtransGAN?color=pink&style=flat-square)](https://github.com/ChenPaulYu/DJtransGAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/ChenPaulYu/DJtransGAN?label=)](https://github.com/ChenPaulYu/DJtransGAN/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/ChenPaulYu/DJtransGAN?label=)](https://github.com/ChenPaulYu/DJtransGAN/graphs/code-frequency)|
|[MixCNN](https://github.com/csteinmetz1/MixCNN#readme)|Mulitrack mix leveling with convolutional neural nets|[![](https://img.shields.io/github/languages/top/csteinmetz1/MixCNN?color=pink&style=flat-square)](https://github.com/csteinmetz1/MixCNN/graphs/contributors)|[![](https://flat.badgen.net/github/license/csteinmetz1/MixCNN?label=)](https://github.com/csteinmetz1/MixCNN/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/csteinmetz1/MixCNN?label=)](https://github.com/csteinmetz1/MixCNN/graphs/code-frequency)|
|[AudMIX](https://github.com/dssudake/AudMIX#readme)|A web-based system for processing Audio using Deep Learning|[![](https://img.shields.io/github/languages/top/dssudake/AudMIX?color=pink&style=flat-square)](https://github.com/dssudake/AudMIX/graphs/contributors)|[![](https://flat.badgen.net/github/license/dssudake/AudMIX?label=)](https://github.com/dssudake/AudMIX/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/dssudake/AudMIX/master?label=)](https://github.com/dssudake/AudMIX/graphs/code-frequency)|
|[deep-audio-mixer](https://github.com/apelykh/deep-audio-mixer#readme)|Deep Learning based system for audio mixing|[![](https://img.shields.io/github/languages/top/apelykh/deep-audio-mixer?color=pink&style=flat-square)](https://github.com/apelykh/deep-audio-mixer/graphs/contributors)|[![](https://flat.badgen.net/github/license/apelykh/deep-audio-mixer?label=)](https://github.com/apelykh/deep-audio-mixer/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/apelykh/deep-audio-mixer/master?label=)](https://github.com/apelykh/deep-audio-mixer/graphs/code-frequency)|

### Source Separation [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[audioss](https://github.com/victor23k/audioss#readme)|Audio source separation tool using a neural network|[![](https://img.shields.io/github/languages/top/victor23k/audioss?color=pink&style=flat-square)](https://github.com/victor23k/audioss/graphs/contributors)|[![](https://flat.badgen.net/github/license/victor23k/audioss?label=)](https://github.com/victor23k/audioss/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/victor23k/audioss?label=)](https://github.com/victor23k/audioss/graphs/code-frequency)|
|[DeepConvSep](https://github.com/MTG/DeepConvSep#readme)|Deep Convolutional Neural Networks for Musical Source Separation|[![](https://img.shields.io/github/languages/top/MTG/DeepConvSep?color=pink&style=flat-square)](https://github.com/MTG/DeepConvSep/graphs/contributors)|[![](https://flat.badgen.net/github/license/MTG/DeepConvSep?label=)](https://github.com/MTG/DeepConvSep/blob/master/COPYING.txt)|[![](https://flat.badgen.net/github/last-commit/MTG/DeepConvSep?label=)](https://github.com/MTG/DeepConvSep/graphs/code-frequency)|
|[Audio Source Separation using Low Latency Neural Network](https://github.com/SConsul/audio-source-separation#readme)|PyTorch code based on "[Monoaural Audio Source Separation Using Deep Convolutional Neural Networks](https://pdfs.semanticscholar.org/fede/f8eedef76692d805a6a3380159a95b79b4de.pdf)" to separate instruments from music using a low-latency neural network|[![](https://img.shields.io/github/languages/top/SConsul/audio-source-separation?color=pink&style=flat-square)](https://github.com/SConsul/audio-source-separation/graphs/contributors)|[![](https://flat.badgen.net/github/license/SConsul/audio-source-separation?label=)](https://github.com/SConsul/audio-source-separation/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/SConsul/audio-source-separation?label=)](https://github.com/SConsul/audio-source-separation/graphs/code-frequency)|
|[A Wavenet for Music Source Separation](https://github.com/francesclluis/source-separation-wavenet#readme)|A neural network for end-to-end music source separation|[![](https://img.shields.io/github/languages/top/francesclluis/source-separation-wavenet?color=pink&style=flat-square)](https://github.com/francesclluis/source-separation-wavenet/graphs/contributors)|[![](https://flat.badgen.net/github/license/francesclluis/source-separation-wavenet?label=)](https://github.com/francesclluis/source-separation-wavenet/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/francesclluis/source-separation-wavenet?label=)](https://github.com/francesclluis/source-separation-wavenet/graphs/code-frequency)|
|[BS-RoFormer](https://github.com/lucidrains/BS-RoFormer#readme)|Implementation of [Band Split Roformer](https://arxiv.org/abs/2309.02612), SOTA Attention network for music source separation|[![](https://img.shields.io/github/languages/top/lucidrains/BS-RoFormer?color=pink&style=flat-square)](https://github.com/lucidrains/BS-RoFormer/graphs/contributors)|[![](https://flat.badgen.net/github/license/lucidrains/BS-RoFormer?label=)](https://github.com/lucidrains/BS-RoFormer/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/lucidrains/BS-RoFormer?label=)](https://github.com/lucidrains/BS-RoFormer/graphs/code-frequency)|
|[GAN_SASS_TF](https://github.com/ahmedassal/GAN_SASS_TF#readme)|TensorFlow implementation of "GAN Single Audio Source Separation"|[![](https://img.shields.io/github/languages/top/ahmedassal/GAN_SASS_TF?color=pink&style=flat-square)](https://github.com/ahmedassal/GAN_SASS_TF/graphs/contributors)|[![](https://flat.badgen.net/github/license/ahmedassal/GAN_SASS_TF?label=)](https://github.com/ahmedassal/GAN_SASS_TF/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/ahmedassal/GAN_SASS_TF?label=)](https://github.com/ahmedassal/GAN_SASS_TF/graphs/code-frequency)|
|[Open-Unmix for PyTorch](https://github.com/sigsep/open-unmix-pytorch#readme)|PyTorch (1.8+) implementation of Open-Unmix, a deep neural network reference implementation for music source separation|[![](https://img.shields.io/github/languages/top/sigsep/open-unmix-pytorch?color=pink&style=flat-square)](https://github.com/sigsep/open-unmix-pytorch/graphs/contributors)|[![](https://flat.badgen.net/github/license/sigsep/open-unmix-pytorch?label=)](https://github.com/sigsep/open-unmix-pytorch/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/sigsep/open-unmix-pytorch?label=)](https://github.com/sigsep/open-unmix-pytorch/graphs/code-frequency)|
|[Ultimate Vocal Remover GUI](https://github.com/Anjok07/ultimatevocalremovergui#readme)|GUI for a Vocal Remover that uses Deep Neural Networks|[![](https://img.shields.io/github/languages/top/Anjok07/ultimatevocalremovergui?color=pink&style=flat-square)](https://github.com/Anjok07/ultimatevocalremovergui/graphs/contributors)|[![](https://flat.badgen.net/github/license/Anjok07/ultimatevocalremovergui?label=)](https://github.com/Anjok07/ultimatevocalremovergui/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/Anjok07/ultimatevocalremovergui?label=)](https://github.com/Anjok07/ultimatevocalremovergui/graphs/code-frequency)|
|[Audio Source Separation](https://github.com/Ankit123Mishra/audio-source-separation#readme)|Deep Neural Network model for Audio source separation|[![](https://img.shields.io/github/languages/top/Ankit123Mishra/audio-source-separation?color=pink&style=flat-square)](https://github.com/Ankit123Mishra/audio-source-separation/graphs/contributors)|[![](https://flat.badgen.net/github/license/Ankit123Mishra/audio-source-separation?label=)](https://github.com/Ankit123Mishra/audio-source-separation/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/Ankit123Mishra/audio-source-separation?label=)](https://github.com/Ankit123Mishra/audio-source-separation/graphs/code-frequency)|
|[CASS](https://github.com/ongyongzheng/cass#readme)|CROSS ADVERSARIAL SOURCE SEPARATION VIA AUTOENCODER|[![](https://img.shields.io/github/languages/top/ongyongzheng/cass?color=pink&style=flat-square)](https://github.com/ongyongzheng/cass/graphs/contributors)|[![](https://flat.badgen.net/github/license/ongyongzheng/cass?label=)](https://github.com/ongyongzheng/cass/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/ongyongzheng/cass?label=)](https://github.com/ongyongzheng/cass/graphs/code-frequency)|
|[GENERATIVE ADVERSARIAL SOURCE SEPARATION](https://github.com/ycemsubakan/sourceseparation_misc#readme)|Generative sourceseparation with GANs|[![](https://img.shields.io/github/languages/top/ycemsubakan/sourceseparation_misc?color=pink&style=flat-square)](https://github.com/ycemsubakan/sourceseparation_misc/graphs/contributors)|[![](https://flat.badgen.net/github/license/ycemsubakan/sourceseparation_misc?label=)](https://github.com/ycemsubakan/sourceseparation_misc/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/ycemsubakan/sourceseparation_misc?label=)](https://github.com/ycemsubakan/sourceseparation_misc/graphs/code-frequency)|


### Fingerprinting [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[neural-audio-fp](https://github.com/mimbres/neural-audio-fp)|Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrasive Learning|![](https://img.shields.io/github/languages/top/mimbres/neural-audio-fp?color=pink&style=flat-square)|![](https://flat.badgen.net/github/license/mimbres/neural-audio-fp?label=)|![](https://flat.badgen.net/github/last-commit/mimbres/neural-audio-fp?label=)|
|[FingerprintDNN](https://github.com/carlmoore256/FingerprintDNN)|Fast pitch detection using a deep neural network trained on audio fingerprints|![](https://img.shields.io/github/languages/top/carlmoore256/FingerprintDNN?color=pink&style=flat-square)|![](https://flat.badgen.net/github/license/carlmoore256/FingerprintDNN?label=)|![](https://flat.badgen.net/github/last-commit/carlmoore256/FingerprintDNN?label=)|
|[pfann](https://github.com/stdio2016/pfann)|Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrasive Learning|![](https://img.shields.io/github/languages/top/stdio2016/pfann?color=pink&style=flat-square)|![](https://flat.badgen.net/github/license/stdio2016/pfann?label=)|![](https://flat.badgen.net/github/last-commit/stdio2016/pfann?label=)|

### Watermarking [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[DNN-audio-watermarking](https://github.com/kosta-pmf/dnn-audio-watermarking#readme)|A robust DNN-based audio watermarking system|[![](https://img.shields.io/github/languages/top/kosta-pmf/dnn-audio-watermarking?color=pink&style=flat-square)](https://github.com/kosta-pmf/dnn-audio-watermarking/graphs/contributors)|[![](https://flat.badgen.net/github/license/kosta-pmf/dnn-audio-watermarking?label=)](https://github.com/kosta-pmf/dnn-audio-watermarking/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/kosta-pmf/dnn-audio-watermarking?label=)](https://github.com/kosta-pmf/dnn-audio-watermarking/graphs/code-frequency)|
|[WavMark](https://github.com/wavmark/wavmark#readme)|AI-based Audio Watermarking Tool|[![](https://img.shields.io/github/languages/top/wavmark/wavmark?color=pink&style=flat-square)](https://github.com/wavmark/wavmark/graphs/contributors)|[![](https://flat.badgen.net/github/license/wavmark/wavmark?label=)](https://github.com/wavmark/wavmark/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/wavmark/wavmark?label=)](https://github.com/wavmark/wavmark/graphs/code-frequency)|
|[IDEAW](https://github.com/PecholaL/IDEAW#readme)|Robust Neural Audio Watermarking with Invertible Dual-Embedding|[![](https://img.shields.io/github/languages/top/PecholaL/IDEAW?color=pink&style=flat-square)](https://github.com/PecholaL/IDEAW/graphs/contributors)|[![](https://flat.badgen.net/github/license/PecholaL/IDEAW?label=)](https://github.com/PecholaL/IDEAW/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/PecholaL/IDEAW?label=)](https://github.com/PecholaL/IDEAW/graphs/code-frequency)|[![](https://flat.badgen.net/github/last-commit/9rg/AutomaticTranscription-viaDL/master?label=)](https://github.com/9rg/AutomaticTranscription-viaDL/graphs/code-frequency)|

### Codecs [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[AudioCodec-Hub](https://github.com/ga642381/AudioCodec-Hub#readme)|A Python library for encoding and decoding audio data, supporting various neural audio codec models|[![](https://img.shields.io/github/languages/top/ga642381/AudioCodec-Hub?color=pink&style=flat-square)](https://github.com/ga642381/AudioCodec-Hub/graphs/contributors)|[![](https://flat.badgen.net/github/license/ga642381/AudioCodec-Hub?label=)](https://github.com/ga642381/AudioCodec-Hub/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/ga642381/AudioCodec-Hub/main?label=)](https://github.com/ga642381/AudioCodec-Hub/graphs/code-frequency)|
|[Descript Audio Codec](https://github.com/descriptinc/descript-audio-codec#readme)|A high fidelity general neural audio codec|[![](https://img.shields.io/github/languages/top/descriptinc/descript-audio-codec?color=pink&style=flat-square)](https://github.com/descriptinc/descript-audio-codec/graphs/contributors)|[![](https://flat.badgen.net/github/license/descriptinc/descript-audio-codec?label=)](https://github.com/descriptinc/descript-audio-codec/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/descriptinc/descript-audio-codec/main?label=)](https://github.com/descriptinc/descript-audio-codec/graphs/code-frequency)|
|[EnCodec](https://github.com/facebookresearch/encodec#readme)|State-of-the-art deep learning based audio codec|[![](https://img.shields.io/github/languages/top/facebookresearch/encodec?color=pink&style=flat-square)](https://github.com/facebookresearch/encodec/graphs/contributors)|[![](https://flat.badgen.net/github/license/facebookresearch/encodec?label=)](https://github.com/facebookresearch/encodec/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/facebookresearch/encodec/main?label=)](https://github.com/facebookresearch/encodec/graphs/code-frequency)|
|[SoundStream](https://github.com/haydenshively/SoundStream#readme)|An end-to-end neural audio codec|[![](https://img.shields.io/github/languages/top/haydenshively/SoundStream?color=pink&style=flat-square)](https://github.com/haydenshively/SoundStream/graphs/contributors)|[![](https://flat.badgen.net/github/license/haydenshively/SoundStream?label=)](https://github.com/haydenshively/SoundStream/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/haydenshively/SoundStream/master?label=)](https://github.com/haydenshively/SoundStream/graphs/code-frequency)|
|[encodec-pytorch](https://github.com/NoFish-528/encodec-pytorch#readme)|unofficial implementation of the [High Fidelity Neural Audio Compression](https://arxiv.org/pdf/2210.13438.pdf)|[![](https://img.shields.io/github/languages/top/NoFish-528/encodec-pytorch?color=pink&style=flat-square)](https://github.com/NoFish-528/encodec-pytorch/graphs/contributors)|[![](https://flat.badgen.net/github/license/NoFish-528/encodec-pytorch?label=)](https://github.com/NoFish-528/encodec-pytorch/blob/main/LICENSE)|[![](https://flat.badgen.net/github/last-commit/NoFish-528/encodec-pytorch/main?label=)](https://github.com/NoFish-528/encodec-pytorch/graphs/code-frequency)|
|[Siamese SIREN](https://github.com/lucala/siamese-siren#readme)|Audio Compression with Implicit Neural Representations|[![](https://img.shields.io/github/languages/top/lucala/siamese-siren?color=pink&style=flat-square)](https://github.com/lucala/siamese-siren/graphs/contributors)|[![](https://flat.badgen.net/github/license/lucala/siamese-siren?label=)](https://github.com/lucala/siamese-siren/blob/main/LICENSE)|[![](https://flat.badgen.net/github/last-commit/lucala/siamese-siren/main?label=)](https://github.com/lucala/siamese-siren/graphs/code-frequency)|
|[Stochastic-Restoration-GAN](https://github.com/abreuwallace/Stochastic-Restoration-GAN#stochastic-restoration-gan)|Stochastic Restoration of Heavily Compressed Musical Audio using Generative Adversarial Networks in Pytorch|[![](https://img.shields.io/github/languages/top/abreuwallace/Stochastic-Restoration-GAN?color=pink&style=flat-square)](https://github.com/abreuwallace/Stochastic-Restoration-GAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/abreuwallace/Stochastic-Restoration-GAN?label=)](https://github.com/abreuwallace/Stochastic-Restoration-GAN/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/abreuwallace/Stochastic-Restoration-GAN?label=)](https://github.com/abreuwallace/Stochastic-Restoration-GAN/graphs/code-frequency)|

### Misc [⌂](#--)
|Name + URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[OpenVINO™ AI Plugins for Audacity](https://github.com/intel/openvino-plugins-ai-audacity#readme)|A set of AI-enabled effects, generators, and analyzers for [Audacity®](https://www.audacityteam.org/)|[![](https://img.shields.io/github/languages/top/intel/openvino-plugins-ai-audacity?color=pink&style=flat-square)](https://github.com/intel/openvino-plugins-ai-audacity/graphs/contributors)|[![](https://flat.badgen.net/github/license/intel/openvino-plugins-ai-audacity?label=)](https://github.com/intel/openvino-plugins-ai-audacity/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/intel/openvino-plugins-ai-audacity?label=)](https://github.com/intel/openvino-plugins-ai-audacity/graphs/code-frequency)|
|[GRAND MATRON](https://github.com/nicholasbulka/grandMatronPlugin#readme)|An audio neural network plugin modeling a low pass filter|[![](https://img.shields.io/github/languages/top/nicholasbulka/grandMatronPlugin?color=pink&style=flat-square)](https://github.com/nicholasbulka/grandMatronPlugin/graphs/contributors)|[![](https://flat.badgen.net/github/license/nicholasbulka/grandMatronPlugin?label=)](https://github.com/nicholasbulka/grandMatronPlugin/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/nicholasbulka/grandMatronPlugin?label=)](https://github.com/nicholasbulka/grandMatronPlugin/graphs/code-frequency)|
|[DRC](https://github.com/Max13245/DRC#readme)|Digital Room Correction (DRC) made with a deep neural network|[![](https://img.shields.io/github/languages/top/Max13245/DRC?color=pink&style=flat-square)](https://github.com/Max13245/DRC/graphs/contributors)|[![](https://flat.badgen.net/github/license/Max13245/DRC?label=)](https://github.com/Max13245/DRC/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/Max13245/DRC?label=)](https://github.com/Max13245/DRC/graphs/code-frequency)|
|[Automatic Transcription via DL](https://github.com/9rg/AutomaticTranscription-viaDL#readme)|Deep learning Japanese instruments - flute and drum - automatic transcription|[![](https://img.shields.io/github/languages/top/9rg/AutomaticTranscription-viaDL?color=pink&style=flat-square)](https://github.com/9rg/AutomaticTranscription-viaDL/graphs/contributors)|[![](https://flat.badgen.net/github/license/9rg/AutomaticTranscription-viaDL?label=)](https://github.com/9rg/AutomaticTranscription-viaDL/blob/master/LICENSE)|
|[Bleeding Removal in Music Signals](https://github.com/its-rajesh/Audio-Bleeding-Removal#readme)|Neural networks for removal of bleeding in music signals for the sequential application of Music Source Separation|[![](https://img.shields.io/github/languages/top/its-rajesh/Audio-Bleeding-Removal?color=pink&style=flat-square)](https://github.com/its-rajesh/Audio-Bleeding-Removal/graphs/contributors)|[![](https://flat.badgen.net/github/license/its-rajesh/Audio-Bleeding-Removal?label=)](https://github.com/its-rajesh/Audio-Bleeding-Removal/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/its-rajesh/Audio-Bleeding-Removal?label=)](https://github.com/its-rajesh/Audio-Bleeding-Removal/graphs/code-frequency)|
|[Musical-Accompaniment-GAN](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN#readme)|Deep Learning project to create a model for accompaniment of piano tracks with guitar, strings, bass and drums|[![](https://img.shields.io/github/languages/top/RaphRozenblum/Musical-Accompaniment-GAN?color=pink&style=flat-square)](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/RaphRozenblum/Musical-Accompaniment-GAN?label=)](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/RaphRozenblum/Musical-Accompaniment-GAN/master?label=)](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN/graphs/code-frequency)|
|[Audio-Denoiser](https://github.com/ChihEnChou/Audio-Denoiser#readme)|Audio Denoiser implemented by CNN and GAN|[![](https://img.shields.io/github/languages/top/ChihEnChou/Audio-Denoiser?color=pink&style=flat-square)](https://github.com/ChihEnChou/Audio-Denoiser/graphs/contributors)|[![](https://flat.badgen.net/github/license/ChihEnChou/Audio-Denoiser?label=)](https://github.com/ChihEnChou/Audio-Denoiser/blob/master/LICENSE)|[![](https://flat.badgen.net/github/last-commit/ChihEnChou/Audio-Denoiser/main?label=)](https://github.com/ChihEnChou/Audio-Denoiser/graphs/code-frequency)|

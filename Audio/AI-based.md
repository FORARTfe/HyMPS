# [![AUDIO](https://flat.badgen.net/badge/HyMPS/AUDIO/green?scale=1.8)](https://github.com/FORARTfe/HyMPS#- "AUDIO section") [![AI-based](https://flat.badgen.net/badge/HyMPS/AI-based%20projects/blue?scale=1.8&label=)](https://github.com/FORARTfe/HyMPS/blob/main/Audio/AI-based.md#-- "AI-based page") <img align="right" alt="stable" src="https://user-images.githubusercontent.com/171307/210727719-14b940a2-d1dc-4991-b6a4-7add74463ce8.png" width="5%" />

### [Voices](#voices-) - [Guitars](#guitars-) - [Bass](#bass-) - [Drums](#drums-) - [Effects](#effects-) - [MIDI](#midi-) - [Mixing](#mixing-) - [Enhancing](#enhancing-) - [Fingerprinting](#fingerprinting-) - [Separating](#separating-) - [Watermarking](#watermarking-) - [Codecs](#codecs-) - [Transcripting](#transcripting-) - [Misc](#misc-)

$\color{orange}\textsf{\Large\&#x24D8;\kern{0.1cm}\small {SORTING: Language (a>z) > License (openness) > Resource (a>z)}}$ 

### Voices [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Kara-Audio](https://github.com/abus-aikorea/kara-audio#readme)|Gradio web-ui for vocal remover that uses [demucs](https://github.com/facebookresearch/demucs#readme) and [mdx-net](https://github.com/kuielab/mdx-net) + automatic subtitle creation using faster [whisper](https://github.com/openai/whisper)|[![](https://img.shields.io/github/languages/top/abus-aikorea/kara-audio?color=pink&style=flat-square)](https://github.com/abus-aikorea/kara-audio/graphs/contributors)|[![](https://flat.badgen.net/github/license/abus-aikorea/kara-audio?label=)](https://github.com/abus-aikorea/kara-audio/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/abus-aikorea/kara-audio?style=flat-square&label=)](https://github.com/abus-aikorea/kara-audio/graphs/code-frequency)|
|[Applio](https://github.com/IAHispano/Applio#readme)|VITS-based Voice Conversion focused on simplicity, quality and performance|[![](https://img.shields.io/github/languages/top/IAHispano/Applio?color=pink&style=flat-square)](https://github.com/IAHispano/Applio/graphs/contributors)|[![](https://flat.badgen.net/github/license/IAHispano/Applio?label=)](https://github.com/IAHispano/Applio/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/IAHispano/Applio?style=flat-square&label=)](https://github.com/IAHispano/Applio/graphs/code-frequency)|
|[OpenVoice](https://github.com/myshell-ai/OpenVoice#readme)|Versatile Instant Voice Cloning ([paper](https://arxiv.org/abs/2312.01479))|[![](https://img.shields.io/github/languages/top/myshell-ai/OpenVoice?color=pink&style=flat-square)](https://github.com/myshell-ai/OpenVoice/graphs/contributors)|[![](https://flat.badgen.net/github/license/myshell-ai/OpenVoice?label=)](https://github.com/myshell-ai/OpenVoice/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/myshell-ai/OpenVoice?style=flat-square&label=)](https://github.com/myshell-ai/OpenVoice/graphs/code-frequency)|
|[Resemble Enhance](https://github.com/resemble-ai/resemble-enhance#readme)|AI powered speech denoising and enhancement|[![](https://img.shields.io/github/languages/top/resemble-ai/resemble-enhance?color=pink&style=flat-square)](https://github.com/resemble-ai/resemble-enhance/graphs/contributors)|[![](https://flat.badgen.net/github/license/resemble-ai/resemble-enhance?label=)](https://github.com/resemble-ai/resemble-enhance/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/resemble-ai/resemble-enhance?style=flat-square&label=)](https://github.com/resemble-ai/resemble-enhance/graphs/code-frequency)|
|[Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning#readme)|Clone a voice in 5 seconds to generate arbitrary speech in real-time|[![](https://img.shields.io/github/languages/top/CorentinJ/Real-Time-Voice-Cloning?color=pink&style=flat-square)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/graphs/contributors)|[![](https://flat.badgen.net/github/license/CorentinJ/Real-Time-Voice-Cloning?label=)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/1291)|[![](https://img.shields.io/github/last-commit/CorentinJ/Real-Time-Voice-Cloning?style=flat-square&label=)](https://github.com/CorentinJ/Real-Time-Voice-Cloning/graphs/code-frequency)|


### Guitars [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[AIDA-X](https://github.com/AidaDSP/AIDA-X#readme)|An Amp Model Player intended to provide high fidelity simulations of amplifiers (entire signal chain) leveraging AI.|[![](https://img.shields.io/github/languages/top/AidaDSP/AIDA-X?color=pink&style=flat-square)](https://github.com/AidaDSP/AIDA-X/graphs/contributors)|[![](https://flat.badgen.net/github/license/AidaDSP/AIDA-X?label=)](https://github.com/AidaDSP/AIDA-X/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/AidaDSP/AIDA-X?style=flat-square&label=)](https://github.com/AidaDSP/AIDA-X/graphs/code-frequency)|
|[Neural Cab](https://github.com/Thiagohgl/neural-cab-audio-plugin#readme)|A FIR guitar cabinet simulator which generates its transfer functions by means of a Variational Auto-Encoder (VAE) trained with an additional adversarial loss and a very simple Boundary Element Method (BEM) simulation to consider the microphone position.|[![](https://img.shields.io/github/languages/top/Thiagohgl/neural-cab-audio-plugin?color=pink&style=flat-square)](https://github.com/Thiagohgl/neural-cab-audio-plugin/graphs/contributors)|[![](https://flat.badgen.net/github/license/Thiagohgl/neural-cab-audio-plugin?label=)](https://github.com/Thiagohgl/neural-cab-audio-plugin/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/Thiagohgl/neural-cab-audio-plugin?style=flat-square&label=)](https://github.com/Thiagohgl/neural-cab-audio-plugin/graphs/code-frequency)|
|[SmartAmpPro](https://github.com/GuitarML/SmartAmpPro#readme)|Guitar plugin made with [JUCE](https://juce.com/) that uses neural network models to emulate real world hardware|[![](https://img.shields.io/github/languages/top/GuitarML/SmartAmpPro?color=pink&style=flat-square)](https://github.com/GuitarML/SmartAmpPro/graphs/contributors)|[![](https://flat.badgen.net/github/license/GuitarML/SmartAmpPro?label=)](https://github.com/GuitarML/SmartAmpPro/blob/main/LICENSE.txt)|[![](https://img.shields.io/github/last-commit/GuitarML/SmartAmpPro?style=flat-square&label=)](https://github.com/GuitarML/SmartAmpPro/graphs/code-frequency)|
|[SmartGuitarAmp](https://github.com/GuitarML/SmartGuitarAmp#readme)|Guitar plugin made with [JUCE](https://juce.com/) that uses neural network models to emulate real world hardware|[![](https://img.shields.io/github/languages/top/GuitarML/SmartGuitarAmp?color=pink&style=flat-square)](https://github.com/GuitarML/SmartGuitarAmp/graphs/contributors)|[![](https://flat.badgen.net/github/license/GuitarML/SmartGuitarAmp?label=)](https://github.com/GuitarML/SmartGuitarAmp/blob/main/LICENSE.txt)|[![](https://img.shields.io/github/last-commit/GuitarML/SmartGuitarAmp?style=flat-square&label=)](https://github.com/GuitarML/SmartGuitarAmp/graphs/code-frequency)|
|[Automated-Guitar Amplifier Modelling](https://github.com/Alec-Wright/Automated-GuitarAmpModelling#readme)|Neural network training scripts and trained models of guitar amplifiers and distortion pedals|[![](https://img.shields.io/github/languages/top/Alec-Wright/Automated-GuitarAmpModelling?color=pink&style=flat-square)](https://github.com/Alec-Wright/Automated-GuitarAmpModelling/graphs/contributors)|[![](https://flat.badgen.net/github/license/Alec-Wright/Automated-GuitarAmpModelling?label=)](https://github.com/Alec-Wright/Automated-GuitarAmpModelling/blob/main/LICENSE.md)|[![](https://img.shields.io/github/last-commit/Alec-Wright/Automated-GuitarAmpModelling?style=flat-square&label=)](https://github.com/Alec-Wright/Automated-GuitarAmpModelling/graphs/code-frequency)|
|[GuitarLSTM](https://github.com/GuitarML/GuitarLSTM#readme)|Deep learning models for guitar amp/pedal emulation using LSTM with [Keras](https://keras.io/)|[![](https://img.shields.io/github/languages/top/GuitarML/GuitarLSTM?color=pink&style=flat-square)](https://github.com/GuitarML/GuitarLSTM/graphs/contributors)|[![](https://flat.badgen.net/github/license/GuitarML/GuitarLSTM?label=)](https://github.com/GuitarML/GuitarLSTM/blob/main/LICENSE.txt)|[![](https://img.shields.io/github/last-commit/GuitarML/GuitarLSTM?style=flat-square&label=)](https://github.com/GuitarML/GuitarLSTM/graphs/code-frequency)|
|[NAM: neural amp modeler](https://github.com/sdatkinson/neural-amp-modeler#readme)|Neural network emulator for guitar amplifiers|[![](https://img.shields.io/github/languages/top/sdatkinson/neural-amp-modeler?color=pink&style=flat-square)](https://github.com/sdatkinson/neural-amp-modeler/graphs/contributors)|[![](https://flat.badgen.net/github/license/sdatkinson/neural-amp-modeler?label=)](https://github.com/sdatkinson/neural-amp-modeler/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/sdatkinson/neural-amp-modeler?style=flat-square&label=)](https://github.com/sdatkinson/neural-amp-modeler/graphs/code-frequency)|
|[CNN Distortion](https://github.com/mganger/cnn-distortion#readme)|Guitar Amp Modeling Plugin and Toolset that combine deep learning and DSP|[![](https://img.shields.io/github/languages/top/mganger/cnn-distortion?color=pink&style=flat-square)](https://github.com/mganger/cnn-distortion/graphs/contributors)|[![](https://flat.badgen.net/github/license/mganger/cnn-distortion?label=)](https://github.com/mganger/cnn-distortion/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/mganger/cnn-distortion?style=flat-square&label=)](https://github.com/mganger/cnn-distortion/graphs/code-frequency)|
|[Deep Guitar Amplifier](https://github.com/salvatorefara/deepGuitarAmp#readme)|A little project to practice [Tensorflow](https://www.tensorflow.org/)/[Keras](https://keras.io/) where deep learning for black-box modelling of a guitar amplifier used|[![](https://img.shields.io/github/languages/top/salvatorefara/deepGuitarAmp?color=pink&style=flat-square)](https://github.com/salvatorefara/deepGuitarAmp/graphs/contributors)|[![](https://flat.badgen.net/github/license/salvatorefara/deepGuitarAmp?label=)](https://github.com/salvatorefara/deepGuitarAmp/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/salvatorefara/deepGuitarAmp?style=flat-square&label=)](https://github.com/salvatorefara/deepGuitarAmp/graphs/code-frequency)|

### Bass [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Walking Bass Transcription](https://github.com/jakobabesser/walking_bass_transcription_dnn#readme)|Algorithm for walking bass transcription in jazz ensemble recordings using Deep Neural Networks (DNN)|[![](https://img.shields.io/github/languages/top/jakobabesser/walking_bass_transcription_dnn?color=pink&style=flat-square)](https://github.com/jakobabesser/walking_bass_transcription_dnn/graphs/contributors)|[![](https://flat.badgen.net/github/license/jakobabesser/walking_bass_transcription_dnn?label=)](https://github.com/jakobabesser/walking_bass_transcription_dnn/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/jakobabesser/walking_bass_transcription_dnn?style=flat-square&label=)](https://github.com/jakobabesser/walking_bass_transcription_dnn/graphs/code-frequency)|
|[BassUNet](https://github.com/jakobabesser/bassunet#readme)|U-Net based convolutional neural network for (jazz) bass transcription|[![](https://img.shields.io/github/languages/top/jakobabesser/bassunet?color=pink&style=flat-square)](https://github.com/jakobabesser/bassunet/graphs/contributors)|[![](https://flat.badgen.net/badge/license/Other/blue?label=)](https://github.com/jakobabesser/bassunet/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/jakobabesser/bassunet?style=flat-square&label=)](https://github.com/jakobabesser/bassunet/graphs/code-frequency)|
|[bassTranscriber](https://github.com/NicholasBlaskey/bassTranscriber#readme)|Automatically transcribing bass lines using neural networks|[![](https://img.shields.io/github/languages/top/NicholasBlaskey/bassTranscriber?color=pink&style=flat-square)](https://github.com/NicholasBlaskey/bassTranscriber/graphs/contributors)|[![](https://flat.badgen.net/github/license/NicholasBlaskey/bassTranscriber?label=)](https://github.com/NicholasBlaskey/bassTranscriber/issues/1)|[![](https://img.shields.io/github/last-commit/NicholasBlaskey/bassTranscriber?style=flat-square&label=)](https://github.com/NicholasBlaskey/bassTranscriber/graphs/code-frequency)|
|[Beatle-Basslines](https://github.com/jmineroff/Beatle-Basslines#readme)|Deep Learning model for creation of an instrument track in a performer's style from Other tracks in a MIDI file|[![](https://img.shields.io/github/languages/top/jmineroff/Beatle-Basslines?color=pink&style=flat-square)](https://github.com/jmineroff/Beatle-Basslines/graphs/contributors)|[![](https://flat.badgen.net/github/license/jmineroff/Beatle-Basslines?label=)](https://github.com/jmineroff/Beatle-Basslines/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/jmineroff/Beatle-Basslines?style=flat-square&label=)](https://github.com/jmineroff/Beatle-Basslines/graphs/code-frequency)|

### Drums [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Automatic Drum Transcription](https://github.com/underson14/automatic-drum-transcription#readme)|Automatic drum transcription using neural nets|[![](https://img.shields.io/github/languages/top/underson14/automatic-drum-transcription?color=pink&style=flat-square)](https://github.com/underson14/automatic-drum-transcription/graphs/contributors)|[![](https://flat.badgen.net/github/license/underson14/automatic-drum-transcription?label=)](https://github.com/underson14/automatic-drum-transcription/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/underson14/automatic-drum-transcription?style=flat-square&label=)](https://github.com/underson14/automatic-drum-transcription/graphs/code-frequency)|
|[Mix-Wave-U-Net](https://github.com/f90/Mix-Wave-U-Net#readme)|Implementation of the [Mix-Wave-U-Net](https://www.aes.org/e-lib/browse.cfm?elib=21023) for automatic mixing of drums|[![](https://img.shields.io/github/languages/top/f90/Mix-Wave-U-Net?color=pink&style=flat-square)](https://github.com/f90/Mix-Wave-U-Net/graphs/contributors)|[![](https://flat.badgen.net/github/license/f90/Mix-Wave-U-Net?label=)](https://github.com/f90/Mix-Wave-U-Net/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/f90/Mix-Wave-U-Net?style=flat-square&label=)](https://github.com/f90/Mix-Wave-U-Net/graphs/code-frequency)|
|[NeuralDrummer](https://github.com/bdshrk/neuraldrummer#readme)|A neural network for generating drum tracks for songs using [Python](https://www.python.org/) and [TensorFlow](https://www.tensorflow.org/)|[![](https://img.shields.io/github/languages/top/bdshrk/neuraldrummer?color=pink&style=flat-square)](https://github.com/bdshrk/neuraldrummer/graphs/contributors)|[![](https://flat.badgen.net/github/license/bdshrk/neuraldrummer?label=)](https://github.com/bdshrk/neuraldrummer/blob/main/LICENSEE)|[![](https://img.shields.io/github/last-commit/bdshrk/neuraldrummer?style=flat-square&label=)](https://github.com/bdshrk/neuraldrummer/graphs/code-frequency)|
|[Neural-Networks-for-Drum-Music-Generation](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation#readme)|Generating realistic drum music using LSTM neural networks trained on rock-style MIDI drum performances|[![](https://img.shields.io/github/languages/top/pareshraut/Neural-Networks-for-Drum-Music-Generation?color=pink&style=flat-square)](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation/graphs/contributors)|[![](https://flat.badgen.net/github/license/pareshraut/Neural-Networks-for-Drum-Music-Generation?label=)](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/pareshraut/Neural-Networks-for-Drum-Music-Generation?style=flat-square&label=)](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation/graphs/code-frequency)|
|[Automatic drums transcription using neural networks](https://github.com/pagrumiaux/drums_transcription#readme)|Internship's code for automatic drums transcription with neural networks|[![](https://img.shields.io/github/languages/top/pagrumiaux/drums_transcription?color=pink&style=flat-square)](https://github.com/pagrumiaux/drums_transcription/graphs/contributors)|[![](https://flat.badgen.net/github/license/pagrumiaux/drums_transcription?label=)](https://github.com/pagrumiaux/drums_transcription/issues/1)|[![](https://img.shields.io/github/last-commit/pagrumiaux/drums_transcription?style=flat-square&label=)](https://github.com/pagrumiaux/drums_transcription/graphs/code-frequency)|
|[drumsep](https://github.com/morehovschi/drumsep#readme)|A Convolutional Neural Network for drum signal separation from full mixes|[![](https://img.shields.io/github/languages/top/morehovschi/drumsep?color=pink&style=flat-square)](https://github.com/morehovschi/drumsep/graphs/contributors)|[![](https://flat.badgen.net/github/license/morehovschi/drumsep?label=)](https://github.com/morehovschi/drumsep/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/morehovschi/drumsep?style=flat-square&label=)](https://github.com/morehovschi/drumsep/graphs/code-frequency)|
|[Generative Adversarial Networks - Drum Pattern Generation](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator#readme)|Generates drum patterns similar to those by [Maciej Kowalski](https://www.metal-archives.com/artists/Maciej_Kowalski/10225/)|[![](https://img.shields.io/github/languages/top/omerkolcak/GANs-Drum-Pattern-Generator?color=pink&style=flat-square)](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator/graphs/contributors)|[![](https://flat.badgen.net/github/license/omerkolcak/GANs-Drum-Pattern-Generator?label=)](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/omerkolcak/GANs-Drum-Pattern-Generator?style=flat-square&label=)](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator/graphs/code-frequency)|

### Effects [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[OpenVINO™ AI Plugins for Audacity](https://github.com/intel/openvino-plugins-ai-audacity#readme)|A set of AI-enabled effects, generators, and analyzers for [Audacity®](https://www.audacityteam.org/)|[![](https://img.shields.io/github/languages/top/intel/openvino-plugins-ai-audacity?color=pink&style=flat-square)](https://github.com/intel/openvino-plugins-ai-audacity/graphs/contributors)|[![](https://flat.badgen.net/github/license/intel/openvino-plugins-ai-audacity?label=)](https://github.com/intel/openvino-plugins-ai-audacity/blob/main/LICENSE.txt)|[![](https://img.shields.io/github/last-commit/intel/openvino-plugins-ai-audacity?style=flat-square&label=)](https://github.com/intel/openvino-plugins-ai-audacity/graphs/code-frequency)|
|[GRAND MATRON](https://github.com/nicholasbulka/grandMatronPlugin#readme)|An audio neural network plugin modeling a low pass filter built in [JUCE](https://juce.com/)|[![](https://img.shields.io/github/languages/top/nicholasbulka/grandMatronPlugin?color=pink&style=flat-square)](https://github.com/nicholasbulka/grandMatronPlugin/graphs/contributors)|[![](https://flat.badgen.net/github/license/nicholasbulka/grandMatronPlugin?label=)](https://github.com/nicholasbulka/grandMatronPlugin/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/nicholasbulka/grandMatronPlugin?style=flat-square&label=)](https://github.com/nicholasbulka/grandMatronPlugin/graphs/code-frequency)|
|[AI Compressor](https://github.com/jamesnapierstuart/19329-AI-Compressor#readme)|Implementing an Intelligent Dynamic Range Compressor Using Machine Learning Approaches|[![](https://img.shields.io/github/languages/top/jamesnapierstuart/19329-AI-Compressor?color=pink&style=flat-square)](https://github.com/jamesnapierstuart/19329-AI-Compressor/graphs/contributors)|[![](https://flat.badgen.net/github/license/jamesnapierstuart/19329-AI-Compressor?label=)](https://github.com/jamesnapierstuart/19329-AI-Compressor/issues/1)|[![](https://img.shields.io/github/last-commit/jamesnapierstuart/19329-AI-Compressor?style=flat-square&label=)](https://github.com/jamesnapierstuart/19329-AI-Compressor/graphs/code-frequency)|

### MIDI [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[NeuralNote](https://github.com/DamRsn/NeuralNote#readme)|Audio Plugin for Audio to MIDI transcription using deep learning|[![](https://img.shields.io/github/languages/top/DamRsn/NeuralNote?color=pink&style=flat-square)](https://github.com/DamRsn/NeuralNote/graphs/contributors)|[![](https://flat.badgen.net/github/license/DamRsn/NeuralNote?label=)](https://github.com/DamRsn/NeuralNote/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/DamRsn/NeuralNote?style=flat-square&label=)](https://github.com/DamRsn/NeuralNote/graphs/code-frequency)|
|[Notochord](https://github.com/Intelligent-Instruments-Lab/notochord#readme)|A real-time neural network model for MIDI performances|[![](https://img.shields.io/github/languages/top/Intelligent-Instruments-Lab/notochord?color=pink&style=flat-square)](https://github.com/Intelligent-Instruments-Lab/notochord/graphs/contributors)|[![](https://flat.badgen.net/github/license/Intelligent-Instruments-Lab/notochord?label=)](https://github.com/Intelligent-Instruments-Lab/notochord/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Intelligent-Instruments-Lab/notochord?style=flat-square&label=)](https://github.com/Intelligent-Instruments-Lab/notochord/graphs/code-frequency)|
|[RoboDrummer](https://github.com/lincolt/RoboDrummer#readme)|Midi drums generator based on deep neural network|[![](https://img.shields.io/github/languages/top/lincolt/RoboDrummer?color=pink&style=flat-square)](https://github.com/lincolt/RoboDrummer/graphs/contributors)|[![](https://flat.badgen.net/github/license/lincolt/RoboDrummer?label=)](https://github.com/lincolt/RoboDrummer/issues/1)|[![](https://img.shields.io/github/last-commit/lincolt/RoboDrummer?style=flat-square&label=)](https://github.com/lincolt/RoboDrummer/graphs/code-frequency)|

### Mixing [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Evolving artificial neural networks for cross-adaptive audio effects](http://iver56.github.io/cross-adaptive-audio/)|Analysis of various features of the audio signal is used to adaptively control parameters of audio processing of the same signal|[![](https://img.shields.io/github/languages/top/iver56/cross-adaptive-audio?color=pink&style=flat-square)](https://github.com/iver56/cross-adaptive-audio/graphs/contributors)|[![](https://flat.badgen.net/github/license/iver56/cross-adaptive-audio?label=)](https://github.com/iver56/cross-adaptive-audio/blob/master/LICENCE)|[![](https://img.shields.io/github/last-commit/iver56/cross-adaptive-audio?style=flat-square&label=)](https://github.com/iver56/cross-adaptive-audio/graphs/code-frequency)|
|[automix-toolkit](https://github.com/csteinmetz1/automix-toolkit#readme)|Models and datasets for training deep learning automatic mixing models|[![](https://img.shields.io/github/languages/top/csteinmetz1/automix-toolkit?color=pink&style=flat-square)](https://github.com/csteinmetz1/automix-toolkit/graphs/contributors)|[![](https://flat.badgen.net/github/license/csteinmetz1/automix-toolkit?label=)](https://github.com/csteinmetz1/automix-toolkit/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/csteinmetz1/automix-toolkit?style=flat-square&label=)](https://github.com/csteinmetz1/automix-toolkit/graphs/code-frequency)|
|[DJtransGAN](https://github.com/ChenPaulYu/DJtransGAN#readme)|Code for "[Automatic DJ Transitions with Differentiable Audio Effects and Generative Adversarial Networks](https://arxiv.org/abs/2110.06525)" paper|[![](https://img.shields.io/github/languages/top/ChenPaulYu/DJtransGAN?color=pink&style=flat-square)](https://github.com/ChenPaulYu/DJtransGAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/ChenPaulYu/DJtransGAN?label=)](https://github.com/ChenPaulYu/DJtransGAN/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/ChenPaulYu/DJtransGAN?style=flat-square&label=)](https://github.com/ChenPaulYu/DJtransGAN/graphs/code-frequency)|
|[MixCNN](https://github.com/csteinmetz1/MixCNN#readme)|Mulitrack mix leveling with convolutional neural nets|[![](https://img.shields.io/github/languages/top/csteinmetz1/MixCNN?color=pink&style=flat-square)](https://github.com/csteinmetz1/MixCNN/graphs/contributors)|[![](https://flat.badgen.net/github/license/csteinmetz1/MixCNN?label=)](https://github.com/csteinmetz1/MixCNN/issues/1)|[![](https://img.shields.io/github/last-commit/csteinmetz1/MixCNN?style=flat-square&label=)](https://github.com/csteinmetz1/MixCNN/graphs/code-frequency)|
|[MachineLearningDrumGainMixing](https://github.com/djmoffat/MachineLearningDrumGainMixing#readme)|Supplementary website with audio examples for [A Machine Learning Approach to Multitrack Gain Mixing of Drums](https://arxiv.org/abs/1810.06603v2) paper|[![](https://img.shields.io/github/languages/top/djmoffat/MachineLearningDrumGainMixing?color=pink&style=flat-square)](https://github.com/djmoffat/MachineLearningDrumGainMixing/graphs/contributors)|[![](https://flat.badgen.net/github/license/djmoffat/MachineLearningDrumGainMixing?label=)](https://github.com/djmoffat/MachineLearningDrumGainMixing/issues/1)|[![](https://img.shields.io/github/last-commit/djmoffat/MachineLearningDrumGainMixing?style=flat-square&label=)](https://github.com/djmoffat/MachineLearningDrumGainMixing/graphs/code-frequency)|
|[AudMIX](https://github.com/dssudake/AudMIX#readme)|A web-based system for processing Audio using Deep Learning|[![](https://img.shields.io/github/languages/top/dssudake/AudMIX?color=pink&style=flat-square)](https://github.com/dssudake/AudMIX/graphs/contributors)|[![](https://flat.badgen.net/github/license/dssudake/AudMIX?label=)](https://github.com/dssudake/AudMIX/issues/9)|[![](https://img.shields.io/github/last-commit/dssudake/AudMIX?style=flat-square&label=)](https://github.com/dssudake/AudMIX/graphs/code-frequency)|
|[deep-audio-mixer](https://github.com/apelykh/deep-audio-mixer#readme)|Deep Learning based system for audio mixing|[![](https://img.shields.io/github/languages/top/apelykh/deep-audio-mixer?color=pink&style=flat-square)](https://github.com/apelykh/deep-audio-mixer/graphs/contributors)|[![](https://flat.badgen.net/github/license/apelykh/deep-audio-mixer?label=)](https://github.com/apelykh/deep-audio-mixer/issues/1)|[![](https://img.shields.io/github/last-commit/apelykh/deep-audio-mixer?style=flat-square&label=)](https://github.com/apelykh/deep-audio-mixer/graphs/code-frequency)|

### Separating [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[audioss](https://github.com/victor23k/audioss#readme)|Audio source separation tool using a neural network|[![](https://img.shields.io/github/languages/top/victor23k/audioss?color=pink&style=flat-square)](https://github.com/victor23k/audioss/graphs/contributors)|[![](https://flat.badgen.net/github/license/victor23k/audioss?label=)](https://github.com/victor23k/audioss/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/victor23k/audioss?style=flat-square&label=)](https://github.com/victor23k/audioss/graphs/code-frequency)|
|[DeepConvSep](https://github.com/MTG/DeepConvSep#readme)|Deep Convolutional Neural Networks for Musical Source Separation|[![](https://img.shields.io/github/languages/top/MTG/DeepConvSep?color=pink&style=flat-square)](https://github.com/MTG/DeepConvSep/graphs/contributors)|[![](https://flat.badgen.net/github/license/MTG/DeepConvSep?label=)](https://github.com/MTG/DeepConvSep/blob/master/COPYING.txt)|[![](https://img.shields.io/github/last-commit/MTG/DeepConvSep?style=flat-square&label=)](https://github.com/MTG/DeepConvSep/graphs/code-frequency)|
|[audio-source-separation](https://github.com/SConsul/audio-source-separation#readme)|[PyTorch](https://pytorch.org/) code for "[Monoaural Audio Source Separation Using Deep Convolutional Neural Networks](https://pdfs.semanticscholar.org/fede/f8eedef76692d805a6a3380159a95b79b4de.pdf)" paper to separate instruments from music using a low-latency neural network|[![](https://img.shields.io/github/languages/top/SConsul/audio-source-separation?color=pink&style=flat-square)](https://github.com/SConsul/audio-source-separation/graphs/contributors)|[![](https://flat.badgen.net/github/license/SConsul/audio-source-separation?label=)](https://github.com/SConsul/audio-source-separation/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/SConsul/audio-source-separation?style=flat-square&label=)](https://github.com/SConsul/audio-source-separation/graphs/code-frequency)|
|[A Wavenet for Music Source Separation](https://github.com/francesclluis/source-separation-wavenet#readme)|A neural network for end-to-end music source separation|[![](https://img.shields.io/github/languages/top/francesclluis/source-separation-wavenet?color=pink&style=flat-square)](https://github.com/francesclluis/source-separation-wavenet/graphs/contributors)|[![](https://flat.badgen.net/github/license/francesclluis/source-separation-wavenet?label=)](https://github.com/francesclluis/source-separation-wavenet/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/francesclluis/source-separation-wavenet?style=flat-square&label=)](https://github.com/francesclluis/source-separation-wavenet/graphs/code-frequency)|
|[BS-RoFormer](https://github.com/lucidrains/BS-RoFormer#readme)|Implementation of [Band Split Roformer](https://arxiv.org/abs/2309.02612), SOTA Attention network for music source separation out of ByteDance AI Labs|[![](https://img.shields.io/github/languages/top/lucidrains/BS-RoFormer?color=pink&style=flat-square)](https://github.com/lucidrains/BS-RoFormer/graphs/contributors)|[![](https://flat.badgen.net/github/license/lucidrains/BS-RoFormer?label=)](https://github.com/lucidrains/BS-RoFormer/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/lucidrains/BS-RoFormer?style=flat-square&label=)](https://github.com/lucidrains/BS-RoFormer/graphs/code-frequency)|
|[Continual Music Source Separation](https://github.com/pedrocg42/continual-music-source-separation#readme)|Repository dedicated to train models for the task of music source separation in the context of continual learning|[![](https://img.shields.io/github/languages/top/pedrocg42/continual-music-source-separation?color=pink&style=flat-square)](https://github.com/pedrocg42/continual-music-source-separation/graphs/contributors)|[![](https://flat.badgen.net/github/license/pedrocg42/continual-music-source-separation?label=)](https://github.com/pedrocg42/continual-music-source-separation/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/pedrocg42/continual-music-source-separation?style=flat-square&label=)](https://github.com/pedrocg42/continual-music-source-separation/graphs/code-frequency)|
|[GAN_SASS_TF](https://github.com/ahmedassal/GAN_SASS_TF#readme)|[TensorFlow](https://www.tensorflow.org/) implementation of "[GAN Single Audio Source Separation](https://hajim.rochester.edu/ece/sites/zduan/teaching/ece472/projects/2019/GANforAudioSourceSeparation.pdf)" paper|[![](https://img.shields.io/github/languages/top/ahmedassal/GAN_SASS_TF?color=pink&style=flat-square)](https://github.com/ahmedassal/GAN_SASS_TF/graphs/contributors)|[![](https://flat.badgen.net/github/license/ahmedassal/GAN_SASS_TF?label=)](https://github.com/ahmedassal/GAN_SASS_TF/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/ahmedassal/GAN_SASS_TF?style=flat-square&label=)](https://github.com/ahmedassal/GAN_SASS_TF/graphs/code-frequency)|
|[Open-Unmix for PyTorch](https://github.com/sigsep/open-unmix-pytorch#readme)|[PyTorch](https://pytorch.org/) (1.8+) implementation of Open-Unmix, a deep neural network reference implementation for music source separation|[![](https://img.shields.io/github/languages/top/sigsep/open-unmix-pytorch?color=pink&style=flat-square)](https://github.com/sigsep/open-unmix-pytorch/graphs/contributors)|[![](https://flat.badgen.net/github/license/sigsep/open-unmix-pytorch?label=)](https://github.com/sigsep/open-unmix-pytorch/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/sigsep/open-unmix-pytorch?style=flat-square&label=)](https://github.com/sigsep/open-unmix-pytorch/graphs/code-frequency)|
|[Ultimate Vocal Remover GUI](https://github.com/Anjok07/ultimatevocalremovergui#readme)|A GUI for a Vocal Remover that uses Deep Neural Networks|[![](https://img.shields.io/github/languages/top/Anjok07/ultimatevocalremovergui?color=pink&style=flat-square)](https://github.com/Anjok07/ultimatevocalremovergui/graphs/contributors)|[![](https://flat.badgen.net/github/license/Anjok07/ultimatevocalremovergui?label=)](https://github.com/Anjok07/ultimatevocalremovergui/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/Anjok07/ultimatevocalremovergui?style=flat-square&label=)](https://github.com/Anjok07/ultimatevocalremovergui/graphs/code-frequency)|
|[Audio Source Separation](https://github.com/Ankit123Mishra/audio-source-separation#readme)|Deep Neural Network model for Audio source separation|[![](https://img.shields.io/github/languages/top/Ankit123Mishra/audio-source-separation?color=pink&style=flat-square)](https://github.com/Ankit123Mishra/audio-source-separation/graphs/contributors)|[![](https://flat.badgen.net/github/license/Ankit123Mishra/audio-source-separation?label=)](https://github.com/Ankit123Mishra/audio-source-separation/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Ankit123Mishra/audio-source-separation?style=flat-square&label=)](https://github.com/Ankit123Mishra/audio-source-separation/graphs/code-frequency)|
|[BandIt: Cinematic Audio Source Separation](https://github.com/karnwatcharasupat/bandit#readme)|Code for "[A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation](https://arxiv.org/abs/2309.02539)" paper|[![](https://img.shields.io/github/languages/top/karnwatcharasupat/bandit?color=pink&style=flat-square)](https://github.com/karnwatcharasupat/bandit/graphs/contributors)|[![](https://flat.badgen.net/github/license/karnwatcharasupat/bandit?label=)](https://github.com/kwatcharasupat/bandit/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/karnwatcharasupat/bandit?style=flat-square&label=)](https://github.com/karnwatcharasupat/bandit/graphs/code-frequency)|
|[CASS](https://github.com/ongyongzheng/cass#readme)|CASS is a source separation model unifying AE and GAN frameworks|[![](https://img.shields.io/github/languages/top/ongyongzheng/cass?color=pink&style=flat-square)](https://github.com/ongyongzheng/cass/graphs/contributors)|[![](https://flat.badgen.net/github/license/ongyongzheng/cass?label=)](https://github.com/ongyongzheng/cass/issues/1)|[![](https://img.shields.io/github/last-commit/ongyongzheng/cass?style=flat-square&label=)](https://github.com/ongyongzheng/cass/graphs/code-frequency)|
|[Generative sourceseparation with GANs](https://github.com/ycemsubakan/sourceseparation_misc#readme)|Code for [Generative Adversarial Source Separation](https://arxiv.org/abs/1710.10779) paper|[![](https://img.shields.io/github/languages/top/ycemsubakan/sourceseparation_misc?color=pink&style=flat-square)](https://github.com/ycemsubakan/sourceseparation_misc/graphs/contributors)|[![](https://flat.badgen.net/github/license/ycemsubakan/sourceseparation_misc?label=)](https://github.com/ycemsubakan/sourceseparation_misc/issues/5)|[![](https://img.shields.io/github/last-commit/ycemsubakan/sourceseparation_misc?style=flat-square&label=)](https://github.com/ycemsubakan/sourceseparation_misc/graphs/code-frequency)|
|[Music Source Separation Universal Training Code](https://github.com/ZFTurbo/Music-Source-Separation-Training#readme)|Repository for training models for music source separation|[![](https://img.shields.io/github/languages/top/ZFTurbo/Music-Source-Separation-Training?color=pink&style=flat-square)](https://github.com/ZFTurbo/Music-Source-Separation-Training/graphs/contributors)|[![](https://flat.badgen.net/github/license/ZFTurbo/Music-Source-Separation-Training?label=)](https://github.com/ZFTurbo/Music-Source-Separation-Training/issues/31)|[![](https://img.shields.io/github/last-commit/ZFTurbo/Music-Source-Separation-Training?style=flat-square&label=)](https://github.com/ZFTurbo/Music-Source-Separation-Training/graphs/code-frequency)|
|[Unmixer](https://github.com/lehenbauer/unmixer#readme)|A GUI frontend for [LALAL.AI](https://lalal.ai/)'s AI-powered stem-splitting technology|[![](https://img.shields.io/github/languages/top/lehenbauer/unmixer?color=pink&style=flat-square)](https://github.com/lehenbauer/unmixer/graphs/contributors)|[![](https://flat.badgen.net/github/license/lehenbauer/unmixer?label=)](https://github.com/lehenbauer/unmixer/issues/1)|[![](https://img.shields.io/github/last-commit/lehenbauer/unmixer?style=flat-square&label=)](https://github.com/lehenbauer/unmixer/graphs/code-frequency)|
|[deeper-wider-melody](https://github.com/drwangxian/deeper-wider-melody#readme)|Code for the "[Enhancing Vocal Melody Extraction with Multilevel Contexts](https://ieeexplore.ieee.org/document/10518111)" paper|[![](https://img.shields.io/github/languages/top/drwangxian/deeper-wider-melody?color=pink&style=flat-square)](https://github.com/drwangxian/deeper-wider-melody/graphs/contributors)|[![](https://flat.badgen.net/github/license/drwangxian/deeper-wider-melody?label=)](https://github.com/drwangxian/deeper-wider-melody/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/drwangxian/deeper-wider-melody?style=flat-square&label=)](https://github.com/drwangxian/deeper-wider-melody/graphs/code-frequency)|

### Enhancing [⌂](#--)
|Name / URL|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Audio-Enhancement-GAN](https://github.com/nimeshrisal/Audio-Enhancement-GAN#readme)|An audio enhancement system which uses GAN|[![](https://img.shields.io/github/languages/top/nimeshrisal/Audio-Enhancement-GAN?color=pink&style=flat-square)](https://github.com/nimeshrisal/Audio-Enhancement-GAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/nimeshrisal/Audio-Enhancement-GAN?label=)](https://github.com/nimeshrisal/Audio-Enhancement-GAN/blob/main/license.md)|[![](https://img.shields.io/github/last-commit/nimeshrisal/Audio-Enhancement-GAN?style=flat-square&label=)](https://github.com/nimeshrisal/Audio-Enhancement-GAN/graphs/code-frequency)|
|[Audio Enhancement with Deep Learning](https://github.com/fszatkowski/Audio-Enhancement-with-Deep-Learning#readme)|Denoising and upsampling audio files using DNN architectures such as autoencoders, SEGAN and WaveNet|[![](https://img.shields.io/github/languages/top/fszatkowski/Audio-Enhancement-with-Deep-Learning?color=pink&style=flat-square)](https://github.com/fszatkowski/Audio-Enhancement-with-Deep-Learning/graphs/contributors)|[![](https://flat.badgen.net/github/license/fszatkowski/Audio-Enhancement-with-Deep-Learning?label=)](https://github.com/fszatkowski/Audio-Enhancement-with-Deep-Learning/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/fszatkowski/Audio-Enhancement-with-Deep-Learning?style=flat-square&label=)](https://github.com/fszatkowski/Audio-Enhancement-with-Deep-Learning/graphs/code-frequency)|
|[AudioSR-DeepUnfold](https://github.com/nicelyblue/audio-sr-deep-unfold#readme)|A deep learning-powered toolkit for super-resolution in sparse microphone arrays|[![](https://img.shields.io/github/languages/top/nicelyblue/audio-sr-deep-unfold?color=pink&style=flat-square)](https://github.com/nicelyblue/audio-sr-deep-unfold/graphs/contributors)|[![](https://flat.badgen.net/github/license/nicelyblue/audio-sr-deep-unfold?label=)](https://github.com/nicelyblue/audio-sr-deep-unfold/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/nicelyblue/audio-sr-deep-unfold?label=)](https://github.com/nicelyblue/audio-sr-deep-unfold/graphs/code-frequency)|
|[AudioSR](https://audioldm.github.io/audiosr/)|Versatile Audio Super-resolution at Scale|[![](https://img.shields.io/github/languages/top/haoheliu/versatile_audio_super_resolution?color=pink&style=flat-square)](https://github.com/haoheliu/versatile_audio_super_resolution/graphs/contributors)|[![](https://flat.badgen.net/github/license/haoheliu/versatile_audio_super_resolution?label=)](https://github.com/haoheliu/versatile_audio_super_resolution/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/haoheliu/versatile_audio_super_resolution?style=flat-square&label=)](https://github.com/haoheliu/versatile_audio_super_resolution/graphs/code-frequency)|
|[AURAL_GAN+predictive_model](https://github.com/Gabeiscool420/AURAL_GAN-predictive_model#readme)|Aims to transform low-quality phone recordings into professional-quality audio using a Generative Adversarial Network (GAN)|[![](https://img.shields.io/github/languages/top/Gabeiscool420/AURAL_GAN-predictive_model?color=pink&style=flat-square)](https://github.com/Gabeiscool420/AURAL_GAN-predictive_model/graphs/contributors)|[![](https://flat.badgen.net/github/license/Gabeiscool420/AURAL_GAN-predictive_model?label=)](https://github.com/Gabeiscool420/AURAL_GAN-predictive_model/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Gabeiscool420/AURAL_GAN-predictive_model?style=flat-square&label=)](https://github.com/Gabeiscool420/AURAL_GAN-predictive_model/graphs/code-frequency)|
|[DeepFilterNet](https://github.com/Rikorose/DeepFilterNet#readme)|A Low Complexity Speech Enhancement Framework for Full-Band Audio (48kHz) using on Deep Filtering|[![](https://img.shields.io/github/languages/top/Rikorose/DeepFilterNet?color=pink&style=flat-square)](https://github.com/Rikorose/DeepFilterNet/graphs/contributors)|[![](https://flat.badgen.net/badge/license/Other/blue?label=)](https://github.com/Rikorose/DeepFilterNet/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Rikorose/DeepFilterNet?style=flat-square&label=)](https://github.com/Rikorose/DeepFilterNet/graphs/code-frequency)|
|[StereoPhaseNet](https://github.com/karisigurd4/StereoPhaseNet#readme)|A deep learning project aimed at correcting phase issues in stereo audio recordings|[![](https://img.shields.io/github/languages/top/karisigurd4/StereoPhaseNet?color=pink&style=flat-square)](https://github.com/karisigurd4/StereoPhaseNet/graphs/contributors)|[![](https://flat.badgen.net/github/license/karisigurd4/StereoPhaseNet?label=)](https://github.com/karisigurd4/StereoPhaseNet/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/karisigurd4/StereoPhaseNet?style=flat-square&label=)](https://github.com/karisigurd4/StereoPhaseNet/graphs/code-frequency)|
|[Stochastic-Restoration-GAN](https://github.com/abreuwallace/Stochastic-Restoration-GAN#readme)|[Stochastic Restoration of Heavily Compressed Musical Audio using Generative Adversarial Networks](https://arxiv.org/abs/2207.01667) in [PyTorch](https://pytorch.org/)|[![](https://img.shields.io/github/languages/top/abreuwallace/Stochastic-Restoration-GAN?color=pink&style=flat-square)](https://github.com/abreuwallace/Stochastic-Restoration-GAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/abreuwallace/Stochastic-Restoration-GAN?label=)](https://github.com/abreuwallace/Stochastic-Restoration-GAN/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/abreuwallace/Stochastic-Restoration-GAN?style=flat-square&label=)](https://github.com/abreuwallace/Stochastic-Restoration-GAN/graphs/code-frequency)|
|[Audio Cycle GAN](https://github.com/gillesdami/audio-cycle-gan#readme)|GAN improving poorly synthesized audio samples|[![](https://img.shields.io/github/languages/top/gillesdami/audio-cycle-gan?color=pink&style=flat-square)](https://github.com/gillesdami/audio-cycle-gan/graphs/contributors)|[![](https://flat.badgen.net/github/license/gillesdami/audio-cycle-gan?label=)](https://github.com/gillesdami/audio-cycle-gan/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/gillesdami/audio-cycle-gan?style=flat-square&label=)](https://github.com/gillesdami/audio-cycle-gan/graphs/code-frequency)|
|[Audio Super Resolution](https://github.com/dsgiitr/Audio-Super-Resolution#readme)|Enhancment of Audio Quality (Bit-Depth and Sampling-Rate) using Deep Learning|[![](https://img.shields.io/github/languages/top/dsgiitr/Audio-Super-Resolution?color=pink&style=flat-square)](https://github.com/dsgiitr/Audio-Super-Resolution/graphs/contributors)|[![](https://flat.badgen.net/github/license/dsgiitr/Audio-Super-Resolution?label=)](https://github.com/gillesdami/audio-cycle-gan/issues/2)|[![](https://img.shields.io/github/last-commit/dsgiitr/Audio-Super-Resolution?style=flat-square&label=)](https://github.com/dsgiitr/Audio-Super-Resolution/graphs/code-frequency)|
|[A-U-NET-Based-Audio-Denoiser](https://github.com/Snig17/A-U-NET-Based-Audio-Denoiser#readme)|A novel approach based on the U-Net architecture to tackle sound noise reduction efficiently|[![](https://img.shields.io/github/languages/top/Snig17/A-U-NET-Based-Audio-Denoiser?color=pink&style=flat-square)](https://github.com/Snig17/A-U-NET-Based-Audio-Denoiser/graphs/contributors)|[![](https://flat.badgen.net/github/license/Snig17/A-U-NET-Based-Audio-Denoiser?label=)](https://github.com/dsgiitr/Audio-Super-Resolution/issues/12)|[![](https://img.shields.io/github/last-commit/Snig17/A-U-NET-Based-Audio-Denoiser?style=flat-square&label=)](https://github.com/Snig17/A-U-NET-Based-Audio-Denoiser/graphs/code-frequency)|
|[GAN_decoded_audio_enhancement](https://github.com/JUiscoming/GAN_decoded_audio_enhancement#readme)|Unofficial PyTorch Implementation of "[Audio codec enhancement with generative adversarial networks](https://arxiv.org/abs/2001.09653)" paper|[![](https://img.shields.io/github/languages/top/JUiscoming/GAN_decoded_audio_enhancement?color=pink&style=flat-square)](https://github.com/JUiscoming/GAN_decoded_audio_enhancement/graphs/contributors)|[![](https://flat.badgen.net/github/license/JUiscoming/GAN_decoded_audio_enhancement?label=)](https://github.com/JUiscoming/GAN_decoded_audio_enhancement/issues/2)|[![](https://img.shields.io/github/last-commit/JUiscoming/GAN_decoded_audio_enhancement?style=flat-square&label=)](https://github.com/JUiscoming/GAN_decoded_audio_enhancement/graphs/code-frequency)|
|[Grosbeak](https://github.com/chrsep/grosbeak#readme)|Audio quality manipulation experiment using deep learning|[![](https://img.shields.io/github/languages/top/chrsep/grosbeak?color=pink&style=flat-square)](https://github.com/chrsep/grosbeak/graphs/contributors)|[![](https://flat.badgen.net/github/license/chrsep/grosbeak?label=)](https://github.com/chrsep/grosbeak/issues/1)|[![](https://img.shields.io/github/last-commit/chrsep/grosbeak?style=flat-square&label=)](https://github.com/chrsep/grosbeak/graphs/code-frequency)|
|[NMGAN](https://github.com/glo-fi/NMGAN#readme)|A GAN designed to upsample low-quality audio|[![](https://img.shields.io/github/languages/top/glo-fi/NMGAN?color=pink&style=flat-square)](https://github.com/glo-fi/NMGAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/glo-fi/NMGAN?label=)](https://github.com/glo-fi/NMGAN/issues/1)|[![](https://img.shields.io/github/last-commit/glo-fi/NMGAN?style=flat-square&label=)](https://github.com/glo-fi/NMGAN/graphs/code-frequency)|
|[Audio-Undistortion](https://github.com/shaws34/Audio-Undistortion#readme)|Audio Signal Processing for Undistorting Audio Clips|[![](https://img.shields.io/github/languages/top/shaws34/Audio-Undistortion?color=pink&style=flat-square)](https://github.com/shaws34/Audio-Undistortion/graphs/contributors)|[![](https://flat.badgen.net/github/license/shaws34/Audio-Undistortion?label=)](https://github.com/shaws34/Audio-Undistortion/issues/10)|[![](https://img.shields.io/github/last-commit/shaws34/Audio-Undistortion?style=flat-square&label=)](https://github.com/shaws34/Audio-Undistortion/graphs/code-frequency)|

### Fingerprinting [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[neural-fingerprinting](https://github.com/chrispla/neural-fingerprinting#readme)|Neural audio fingerprinting model similar to "[Neural audio fingerprint for high-specific audio retrieval based on contrastive learning](https://arxiv.org/abs/2010.11910)" but with a more lightweight encoder, simpler nearest neighbor search, and implemented in [PyTorch](https://pytorch.org/)|[![](https://img.shields.io/github/languages/top/chrispla/neural-fingerprinting?color=pink&style=flat-square)](https://github.com/chrispla/neural-fingerprinting/graphs/contributors)|[![](https://flat.badgen.net/github/license/chrispla/neural-fingerprinting?label=)](https://github.com/chrispla/neural-fingerprinting/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/chrispla/neural-fingerprinting?style=flat-square&label=)](https://github.com/chrispla/neural-fingerprinting/graphs/code-frequency)|
|[neural-audio-fp](https://mimbres.github.io/neural-audio-fp/)|[Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrasive Learning](https://arxiv.org/abs/2010.11910)|[![](https://img.shields.io/github/languages/top/mimbres/neural-audio-fp?color=pink&style=flat-square)](https://github.com/mimbres/neural-audio-fp/graphs/contributors)|[![](https://flat.badgen.net/github/license/mimbres/neural-audio-fp?label=)](https://github.com/mimbres/neural-audio-fp/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/mimbres/neural-audio-fp?style=flat-square&label=)](https://github.com/mimbres/neural-audio-fp/graphs/code-frequency)|
|[FingerprintDNN](https://github.com/carlmoore256/FingerprintDNN#readme)|Fast pitch detection using a deep neural network trained on audio fingerprints|[![](https://img.shields.io/github/languages/top/carlmoore256/FingerprintDNN?color=pink&style=flat-square)](https://github.com/carlmoore256/FingerprintDNN/graphs/contributors)|[![](https://flat.badgen.net/github/license/carlmoore256/FingerprintDNN?label=)](https://github.com/carlmoore256/FingerprintDNN/issues/1)|[![](https://img.shields.io/github/last-commit/carlmoore256/FingerprintDNN?style=flat-square&label=)](https://github.com/carlmoore256/FingerprintDNN/graphs/code-frequency)|
|[pfann](https://github.com/stdio2016/pfann#readme)|Unofficial reproduction of "[Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrasive Learning](https://arxiv.org/abs/2010.11910)" paper|[![](https://img.shields.io/github/languages/top/stdio2016/pfann?color=pink&style=flat-square)](https://github.com/stdio2016/pfann/graphs/contributors)|[![](https://flat.badgen.net/github/license/stdio2016/pfann?label=)](https://github.com/stdio2016/pfann/issues/3)|[![](https://img.shields.io/github/last-commit/stdio2016/pfann?style=flat-square&label=)](https://github.com/stdio2016/pfann/graphs/code-frequency)|

### Watermarking [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[DNN-audio-watermarking](https://github.com/kosta-pmf/dnn-audio-watermarking#readme)|A robust DNN-based audio watermarking system|[![](https://img.shields.io/github/languages/top/kosta-pmf/dnn-audio-watermarking?color=pink&style=flat-square)](https://github.com/kosta-pmf/dnn-audio-watermarking/graphs/contributors)|[![](https://flat.badgen.net/github/license/kosta-pmf/dnn-audio-watermarking?label=)](https://github.com/kosta-pmf/dnn-audio-watermarking/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/kosta-pmf/dnn-audio-watermarking?style=flat-square&label=)](https://github.com/kosta-pmf/dnn-audio-watermarking/graphs/code-frequency)|
|[WavMark](https://github.com/wavmark/wavmark#readme)|AI-based Audio Watermarking Tool|[![](https://img.shields.io/github/languages/top/wavmark/wavmark?color=pink&style=flat-square)](https://github.com/wavmark/wavmark/graphs/contributors)|[![](https://flat.badgen.net/github/license/wavmark/wavmark?label=)](https://github.com/wavmark/wavmark/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/wavmark/wavmark?style=flat-square&label=)](https://github.com/wavmark/wavmark/graphs/code-frequency)|
|[IDEAW](https://github.com/PecholaL/IDEAW#readme)|Robust Neural Audio Watermarking with Invertible Dual-Embedding|[![](https://img.shields.io/github/languages/top/PecholaL/IDEAW?color=pink&style=flat-square)](https://github.com/PecholaL/IDEAW/graphs/contributors)|[![](https://flat.badgen.net/github/license/PecholaL/IDEAW?label=)](https://github.com/PecholaL/IDEAW/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/PecholaL/IDEAW?style=flat-square&label=)](https://github.com/PecholaL/IDEAW/graphs/code-frequency)|[![](https://img.shields.io/github/last-commit/9rg/AutomaticTranscription-viaDL/master?label=)](https://github.com/9rg/AutomaticTranscription-viaDL/graphs/code-frequency)|

### Codecs [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[AudioCodec-Hub](https://github.com/ga642381/AudioCodec-Hub#readme)|A Python library for encoding and decoding audio data, supporting various neural audio codec models|[![](https://img.shields.io/github/languages/top/ga642381/AudioCodec-Hub?color=pink&style=flat-square)](https://github.com/ga642381/AudioCodec-Hub/graphs/contributors)|[![](https://flat.badgen.net/github/license/ga642381/AudioCodec-Hub?label=)](https://github.com/ga642381/AudioCodec-Hub/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/ga642381/AudioCodec-Hub/main?style=flat-square&label=)](https://github.com/ga642381/AudioCodec-Hub/graphs/code-frequency)|
|[Descript Audio Codec](https://github.com/descriptinc/descript-audio-codec#readme)|A high fidelity general neural audio codec|[![](https://img.shields.io/github/languages/top/descriptinc/descript-audio-codec?color=pink&style=flat-square)](https://github.com/descriptinc/descript-audio-codec/graphs/contributors)|[![](https://flat.badgen.net/github/license/descriptinc/descript-audio-codec?label=)](https://github.com/descriptinc/descript-audio-codec/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/descriptinc/descript-audio-codec/main?style=flat-square&label=)](https://github.com/descriptinc/descript-audio-codec/graphs/code-frequency)|
|[EnCodec](https://github.com/facebookresearch/encodec#readme)|State-of-the-art deep learning based audio codec|[![](https://img.shields.io/github/languages/top/facebookresearch/encodec?color=pink&style=flat-square)](https://github.com/facebookresearch/encodec/graphs/contributors)|[![](https://flat.badgen.net/github/license/facebookresearch/encodec?label=)](https://github.com/facebookresearch/encodec/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/facebookresearch/encodec/main?style=flat-square&label=)](https://github.com/facebookresearch/encodec/graphs/code-frequency)|
|[encodec-pytorch](https://github.com/NoFish-528/encodec-pytorch#readme)|unofficial implementation of the [High Fidelity Neural Audio Compression](https://arxiv.org/pdf/2210.13438.pdf)|[![](https://img.shields.io/github/languages/top/NoFish-528/encodec-pytorch?color=pink&style=flat-square)](https://github.com/NoFish-528/encodec-pytorch/graphs/contributors)|[![](https://flat.badgen.net/github/license/NoFish-528/encodec-pytorch?label=)](https://github.com/ZhikangNiu/encodec-pytorch/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/NoFish-528/encodec-pytorch/main?style=flat-square&label=)](https://github.com/NoFish-528/encodec-pytorch/graphs/code-frequency)|
|[NeuralAudio](https://github.com/mattminuti/NeuralAudio#readme)|Sound compression based on Growing Self-Organizing Maps|[![](https://img.shields.io/github/languages/top/mattminuti/NeuralAudio?color=pink&style=flat-square)](https://github.com/mattminuti/NeuralAudio/graphs/contributors)|[![](https://flat.badgen.net/github/license/mattminuti/NeuralAudio?label=)](https://github.com/mattminuti/NeuralAudio/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/mattminuti/NeuralAudio?style=flat-square&label=)](https://github.com/mattminuti/NeuralAudio/graphs/code-frequency)|
|[Siamese SIREN](https://github.com/lucala/siamese-siren#readme)|Audio Compression with Implicit Neural Representations|[![](https://img.shields.io/github/languages/top/lucala/siamese-siren?color=pink&style=flat-square)](https://github.com/lucala/siamese-siren/graphs/contributors)|[![](https://flat.badgen.net/github/license/lucala/siamese-siren?label=)](https://github.com/lucala/siamese-siren/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/lucala/siamese-siren/main?style=flat-square&label=)](https://github.com/lucala/siamese-siren/graphs/code-frequency)|
|[SoundStream](https://github.com/haydenshively/SoundStream#readme)|An end-to-end neural audio codec|[![](https://img.shields.io/github/languages/top/haydenshively/SoundStream?color=pink&style=flat-square)](https://github.com/haydenshively/SoundStream/graphs/contributors)|[![](https://flat.badgen.net/github/license/haydenshively/SoundStream?label=)](https://github.com/haydenshively/SoundStream/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/haydenshively/SoundStream/master?style=flat-square&label=)](https://github.com/haydenshively/SoundStream/graphs/code-frequency)|
|[audiolite](https://github.com/samarthagali/audiolite#readme)|A deep learning framework for audio compression and decompression|[![](https://img.shields.io/github/languages/top/samarthagali/audiolite?color=pink&style=flat-square)](https://github.com/samarthagali/audiolite/graphs/contributors)|[![](https://flat.badgen.net/github/license/samarthagali/audiolite?label=)](https://github.com/samarthagali/audiolite/issues/2)|[![](https://img.shields.io/github/last-commit/samarthagali/audiolite?style=flat-square&label=)](https://github.com/samarthagali/audiolite/graphs/code-frequency)|

### Transcripting [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[audioFlux](https://github.com/libAudioFlux/audioFlux#readme)|A deep learning tool library for audio and music analysis, feature extraction|[![](https://img.shields.io/github/languages/top/libAudioFlux/audioFlux?color=pink&style=flat-square)](https://github.com/libAudioFlux/audioFlux/graphs/contributors)|[![](https://flat.badgen.net/github/license/libAudioFlux/audioFlux?label=)](https://github.com/libAudioFlux/audioFlux/blob/master/LICENSE.md)|[![](https://img.shields.io/github/last-commit/libAudioFlux/audioFlux?style=flat-square&label=)](https://github.com/libAudioFlux/audioFlux/graphs/code-frequency)|
|[AutomaticTranscription-viaDL](https://github.com/9rg/AutomaticTranscription-viaDL#readme)|Deep learning Japanese instruments - flute and drum - automatic transcription|[![](https://img.shields.io/github/languages/top/9rg/AutomaticTranscription-viaDL?color=pink&style=flat-square)](https://github.com/9rg/AutomaticTranscription-viaDL/graphs/contributors)|[![](https://flat.badgen.net/github/license/9rg/AutomaticTranscription-viaDL?label=)](https://github.com/9rg/AutomaticTranscription-viaDL/issues/1)|[![](https://img.shields.io/github/last-commit/9rg/AutomaticTranscription-viaDL?style=flat-square&label=)](https://github.com/9rg/AutomaticTranscription-viaDL/graphs/code-frequency)|

### Misc [⌂](#--)
|Resource|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[whisper.cpp](https://github.com/ggerganov/whisper.cpp#readme)|High-performance inference of [OpenAI's Whisper](https://github.com/openai/whisper) automatic speech recognition (ASR) model|[![](https://img.shields.io/github/languages/top/ggerganov/whisper.cpp?color=pink&style=flat-square)](https://github.com/ggerganov/whisper.cpp/graphs/contributors)|[![](https://flat.badgen.net/github/license/ggerganov/whisper.cpp?label=)](https://github.com/ggerganov/whisper.cpp/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/ggerganov/whisper.cpp?style=flat-square&label=)](https://github.com/ggerganov/whisper.cpp/graphs/code-frequency)|
|[DRC](https://github.com/Max13245/DRC#readme)|Digital Room Correction (DRC) made with a deep neural network|[![](https://img.shields.io/github/languages/top/Max13245/DRC?color=pink&style=flat-square)](https://github.com/Max13245/DRC/graphs/contributors)|[![](https://flat.badgen.net/github/license/Max13245/DRC?label=)](https://github.com/Max13245/DRC/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/Max13245/DRC?style=flat-square&label=)](https://github.com/Max13245/DRC/graphs/code-frequency)|
|[Bleeding Removal in Music Signals](https://github.com/its-rajesh/Audio-Bleeding-Removal#readme)|Neural networks for removal of bleeding in music signals for the sequential application of Music Source Separation|[![](https://img.shields.io/github/languages/top/its-rajesh/Audio-Bleeding-Removal?color=pink&style=flat-square)](https://github.com/its-rajesh/Audio-Bleeding-Removal/graphs/contributors)|[![](https://flat.badgen.net/github/license/its-rajesh/Audio-Bleeding-Removal?label=)](https://github.com/its-rajesh/Audio-Bleeding-Removal/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/its-rajesh/Audio-Bleeding-Removal?style=flat-square&label=)](https://github.com/its-rajesh/Audio-Bleeding-Removal/graphs/code-frequency)|
|[Musical-Accompaniment-GAN](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN#readme)|Deep Learning project to create a model for accompaniment of piano tracks with guitar, strings, bass and drums|[![](https://img.shields.io/github/languages/top/RaphRozenblum/Musical-Accompaniment-GAN?color=pink&style=flat-square)](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/RaphRozenblum/Musical-Accompaniment-GAN?label=)](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN/issues/1)|[![](https://img.shields.io/github/last-commit/RaphRozenblum/Musical-Accompaniment-GAN/master?style=flat-square&label=)](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN/graphs/code-frequency)|
|[Audio-Denoiser](https://github.com/ChihEnChou/Audio-Denoiser#readme)|Audio Denoiser implemented by CNN and GAN|[![](https://img.shields.io/github/languages/top/ChihEnChou/Audio-Denoiser?color=pink&style=flat-square)](https://github.com/ChihEnChou/Audio-Denoiser/graphs/contributors)|[![](https://flat.badgen.net/github/license/ChihEnChou/Audio-Denoiser?label=)](https://github.com/ChihEnChou/Audio-Denoiser/issues/1)|[![](https://img.shields.io/github/last-commit/ChihEnChou/Audio-Denoiser/main?style=flat-square&label=)](https://github.com/ChihEnChou/Audio-Denoiser/graphs/code-frequency)|


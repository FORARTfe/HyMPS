# [![AUDIO](https://flat.badgen.net/badge/HyMPS/AUDIO/green?scale=1.8)](https://github.com/FORARTfe/HyMPS#- "AUDIO section") [![AI-based](https://flat.badgen.net/badge/HyMPS/AI-based%20projects/blue?scale=1.8&label=)](https://github.com/FORARTfe/HyMPS/blob/main/Audio/AI-based.md#-- "AI-based page") <a href="https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2FFORARTfe%2FHyMPS%2Fblob%2Fmain%2FAudio%2FAI-based.md"><img align="right" src="https://api.visitorbadge.io/api/combined?path=https%3A%2F%2Fgithub.com%2FFORARTfe%2FHyMPS%2Fblob%2Fmain%2FAudio%2FAI-based.md&label=D%20%2F%20T&labelColor=%23323232&countColor=%23c2ff00&style=flat-square&labelStyle=none" /></a>

### ðŸ—‚ï¸ [Voicing](https://github.com/FORARTfe/HyMPS/blob/main/Audio/AI-Voicing.md#---) - [Guitars](#guitars-) - [Bass](#bass-) - [Drums](#drums-) - [Effects](#effects-) - [MIDI](#midi-) - [Mixing/Mastering](#mixingmastering-) - [Enhancing](https://github.com/FORARTfe/HyMPS/blob/main/Audio/AI-Enhancing.md#---) - [Fingerprinting](#fingerprinting-) - [Separating](#separating-) - [Watermarking](#watermarking-) - [Codecs](#codecs-) - [Transcripting](#transcripting-) - [Misc](#misc-)

> [!WARNING]
> $\color{orange}\textsf{{SORTING: Language (a>z) > License (openness) > Repository (a>z)}}$

### Guitars [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[AIDA-X](https://github.com/AidaDSP/AIDA-X#readme)|An Amp Model Player intended to provide high fidelity simulations of amplifiers (entire signal chain) leveraging AI.|[![](https://img.shields.io/github/languages/top/AidaDSP/AIDA-X?color=pink&style=flat-square)](https://github.com/AidaDSP/AIDA-X/graphs/contributors)|[![](https://flat.badgen.net/github/license/AidaDSP/AIDA-X?label=)](https://github.com/AidaDSP/AIDA-X/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/AidaDSP/AIDA-X?style=flat-square&label=)](https://github.com/AidaDSP/AIDA-X/graphs/code-frequency)|
|[Neural Cab](https://github.com/Thiagohgl/neural-cab-audio-plugin#readme)|A FIR guitar cabinet simulator which generates its transfer functions by means of a Variational Auto-Encoder (VAE) trained with an additional adversarial loss and a very simple Boundary Element Method (BEM) simulation to consider the microphone position.|[![](https://img.shields.io/github/languages/top/Thiagohgl/neural-cab-audio-plugin?color=pink&style=flat-square)](https://github.com/Thiagohgl/neural-cab-audio-plugin/graphs/contributors)|[![](https://flat.badgen.net/github/license/Thiagohgl/neural-cab-audio-plugin?label=)](https://github.com/Thiagohgl/neural-cab-audio-plugin/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Thiagohgl/neural-cab-audio-plugin?style=flat-square&label=)](https://github.com/Thiagohgl/neural-cab-audio-plugin/graphs/code-frequency)|
|[SmartAmpPro](https://github.com/GuitarML/SmartAmpPro#readme)|Guitar plugin made with [JUCE](https://juce.com/) that uses neural network models to emulate real world hardware|[![](https://img.shields.io/github/languages/top/GuitarML/SmartAmpPro?color=pink&style=flat-square)](https://github.com/GuitarML/SmartAmpPro/graphs/contributors)|[![](https://flat.badgen.net/github/license/GuitarML/SmartAmpPro?label=)](https://github.com/GuitarML/SmartAmpPro/blob/main/LICENSE.txt)|[![](https://img.shields.io/github/last-commit/GuitarML/SmartAmpPro?style=flat-square&label=)](https://github.com/GuitarML/SmartAmpPro/graphs/code-frequency)|
|[SmartGuitarAmp](https://github.com/GuitarML/SmartGuitarAmp#readme)|Guitar plugin made with [JUCE](https://juce.com/) that uses neural network models to emulate real world hardware|[![](https://img.shields.io/github/languages/top/GuitarML/SmartGuitarAmp?color=pink&style=flat-square)](https://github.com/GuitarML/SmartGuitarAmp/graphs/contributors)|[![](https://flat.badgen.net/github/license/GuitarML/SmartGuitarAmp?label=)](https://github.com/GuitarML/SmartGuitarAmp/blob/main/LICENSE.txt)|[![](https://img.shields.io/github/last-commit/GuitarML/SmartGuitarAmp?style=flat-square&label=)](https://github.com/GuitarML/SmartGuitarAmp/graphs/code-frequency)|
|[CNN Distortion](https://github.com/mganger/cnn-distortion#readme)|Guitar Amp Modeling Plugin and Toolset that combine deep learning and DSP|[![](https://img.shields.io/github/languages/top/mganger/cnn-distortion?color=pink&style=flat-square)](https://github.com/mganger/cnn-distortion/graphs/contributors)|[![](https://flat.badgen.net/github/license/mganger/cnn-distortion?label=)](https://github.com/mganger/cnn-distortion/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/mganger/cnn-distortion?style=flat-square&label=)](https://github.com/mganger/cnn-distortion/graphs/code-frequency)|
|[Deep Guitar Amplifier](https://github.com/salvatorefara/deepGuitarAmp#readme)|A little project to practice [Tensorflow](https://www.tensorflow.org/)/[Keras](https://keras.io/) where deep learning for black-box modelling of a guitar amplifier used|[![](https://img.shields.io/github/languages/top/salvatorefara/deepGuitarAmp?color=pink&style=flat-square)](https://github.com/salvatorefara/deepGuitarAmp/graphs/contributors)|[![](https://flat.badgen.net/github/license/salvatorefara/deepGuitarAmp?label=)](https://github.com/salvatorefara/deepGuitarAmp/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/salvatorefara/deepGuitarAmp?style=flat-square&label=)](https://github.com/salvatorefara/deepGuitarAmp/graphs/code-frequency)|
|[Automated-Guitar Amplifier Modelling](https://github.com/Alec-Wright/Automated-GuitarAmpModelling#readme)|Neural network training scripts and trained models of guitar amplifiers and distortion pedals|[![](https://img.shields.io/github/languages/top/Alec-Wright/Automated-GuitarAmpModelling?color=pink&style=flat-square)](https://github.com/Alec-Wright/Automated-GuitarAmpModelling/graphs/contributors)|[![](https://flat.badgen.net/github/license/Alec-Wright/Automated-GuitarAmpModelling?label=)](https://github.com/Alec-Wright/Automated-GuitarAmpModelling/blob/main/LICENSE.md)|[![](https://img.shields.io/github/last-commit/Alec-Wright/Automated-GuitarAmpModelling?style=flat-square&label=)](https://github.com/Alec-Wright/Automated-GuitarAmpModelling/graphs/code-frequency)|
|[GuitarLSTM](https://github.com/GuitarML/GuitarLSTM#readme)|Deep learning models for guitar amp/pedal emulation using LSTM with [Keras](https://keras.io/)|[![](https://img.shields.io/github/languages/top/GuitarML/GuitarLSTM?color=pink&style=flat-square)](https://github.com/GuitarML/GuitarLSTM/graphs/contributors)|[![](https://flat.badgen.net/github/license/GuitarML/GuitarLSTM?label=)](https://github.com/GuitarML/GuitarLSTM/blob/main/LICENSE.txt)|[![](https://img.shields.io/github/last-commit/GuitarML/GuitarLSTM?style=flat-square&label=)](https://github.com/GuitarML/GuitarLSTM/graphs/code-frequency)|
|[NAM: neural amp modeler](https://github.com/sdatkinson/neural-amp-modeler#readme)|Neural network emulator for guitar amplifiers|[![](https://img.shields.io/github/languages/top/sdatkinson/neural-amp-modeler?color=pink&style=flat-square)](https://github.com/sdatkinson/neural-amp-modeler/graphs/contributors)|[![](https://flat.badgen.net/github/license/sdatkinson/neural-amp-modeler?label=)](https://github.com/sdatkinson/neural-amp-modeler/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/sdatkinson/neural-amp-modeler?style=flat-square&label=)](https://github.com/sdatkinson/neural-amp-modeler/graphs/code-frequency)|


### Bass [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[Beatle-Basslines](https://github.com/jmineroff/Beatle-Basslines#readme)|Deep Learning model for creation of an instrument track in a performer's style from Other tracks in a MIDI file|[![](https://img.shields.io/github/languages/top/jmineroff/Beatle-Basslines?color=pink&style=flat-square)](https://github.com/jmineroff/Beatle-Basslines/graphs/contributors)|[![](https://flat.badgen.net/github/license/jmineroff/Beatle-Basslines?label=)](https://github.com/jmineroff/Beatle-Basslines/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/jmineroff/Beatle-Basslines?style=flat-square&label=)](https://github.com/jmineroff/Beatle-Basslines/graphs/code-frequency)|
|[Walking Bass Transcription](https://github.com/jakobabesser/walking_bass_transcription_dnn#readme)|Algorithm for walking bass transcription in jazz ensemble recordings using Deep Neural Networks (DNN)|[![](https://img.shields.io/github/languages/top/jakobabesser/walking_bass_transcription_dnn?color=pink&style=flat-square)](https://github.com/jakobabesser/walking_bass_transcription_dnn/graphs/contributors)|[![](https://flat.badgen.net/github/license/jakobabesser/walking_bass_transcription_dnn?label=)](https://github.com/jakobabesser/walking_bass_transcription_dnn/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/jakobabesser/walking_bass_transcription_dnn?style=flat-square&label=)](https://github.com/jakobabesser/walking_bass_transcription_dnn/graphs/code-frequency)|
|[BassUNet](https://github.com/jakobabesser/bassunet#readme)|U-Net based convolutional neural network for (jazz) bass transcription|[![](https://img.shields.io/github/languages/top/jakobabesser/bassunet?color=pink&style=flat-square)](https://github.com/jakobabesser/bassunet/graphs/contributors)|[![](https://flat.badgen.net/badge/license/Other/blue?label=)](https://github.com/jakobabesser/bassunet/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/jakobabesser/bassunet?style=flat-square&label=)](https://github.com/jakobabesser/bassunet/graphs/code-frequency)|
|[bassTranscriber](https://github.com/NicholasBlaskey/bassTranscriber#readme)|Automatically transcribing bass lines using neural networks|[![](https://img.shields.io/github/languages/top/NicholasBlaskey/bassTranscriber?color=pink&style=flat-square)](https://github.com/NicholasBlaskey/bassTranscriber/graphs/contributors)|[![](https://flat.badgen.net/github/license/NicholasBlaskey/bassTranscriber?label=)](https://github.com/NicholasBlaskey/bassTranscriber/issues/1)|[![](https://img.shields.io/github/last-commit/NicholasBlaskey/bassTranscriber?style=flat-square&label=)](https://github.com/NicholasBlaskey/bassTranscriber/graphs/code-frequency)|


### Drums [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[drumsep](https://github.com/morehovschi/drumsep#readme)|A Convolutional Neural Network for drum signal separation from full mixes|[![](https://img.shields.io/github/languages/top/morehovschi/drumsep?color=pink&style=flat-square)](https://github.com/morehovschi/drumsep/graphs/contributors)|[![](https://flat.badgen.net/github/license/morehovschi/drumsep?label=)](https://github.com/morehovschi/drumsep/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/morehovschi/drumsep?style=flat-square&label=)](https://github.com/morehovschi/drumsep/graphs/code-frequency)|
|[Generative Adversarial Networks - Drum Pattern Generation](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator#readme)|Generates drum patterns similar to those by [Maciej Kowalski](https://www.metal-archives.com/artists/Maciej_Kowalski/10225/)|[![](https://img.shields.io/github/languages/top/omerkolcak/GANs-Drum-Pattern-Generator?color=pink&style=flat-square)](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator/graphs/contributors)|[![](https://flat.badgen.net/github/license/omerkolcak/GANs-Drum-Pattern-Generator?label=)](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/omerkolcak/GANs-Drum-Pattern-Generator?style=flat-square&label=)](https://github.com/omerkolcak/GANs-Drum-Pattern-Generator/graphs/code-frequency)|
|[Automatic Drum Transcription](https://github.com/underson14/automatic-drum-transcription#readme)|Automatic drum transcription using neural nets|[![](https://img.shields.io/github/languages/top/underson14/automatic-drum-transcription?color=pink&style=flat-square)](https://github.com/underson14/automatic-drum-transcription/graphs/contributors)|[![](https://flat.badgen.net/github/license/underson14/automatic-drum-transcription?label=)](https://github.com/underson14/automatic-drum-transcription/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/underson14/automatic-drum-transcription?style=flat-square&label=)](https://github.com/underson14/automatic-drum-transcription/graphs/code-frequency)|
|[Mix-Wave-U-Net](https://github.com/f90/Mix-Wave-U-Net#readme)|Implementation of the [Mix-Wave-U-Net](https://www.aes.org/e-lib/browse.cfm?elib=21023) for automatic mixing of drums|[![](https://img.shields.io/github/languages/top/f90/Mix-Wave-U-Net?color=pink&style=flat-square)](https://github.com/f90/Mix-Wave-U-Net/graphs/contributors)|[![](https://flat.badgen.net/github/license/f90/Mix-Wave-U-Net?label=)](https://github.com/f90/Mix-Wave-U-Net/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/f90/Mix-Wave-U-Net?style=flat-square&label=)](https://github.com/f90/Mix-Wave-U-Net/graphs/code-frequency)|
|[NeuralDrummer](https://github.com/bdshrk/neuraldrummer#readme)|A neural network for generating drum tracks for songs using [Python](https://www.python.org/) and [TensorFlow](https://www.tensorflow.org/)|[![](https://img.shields.io/github/languages/top/bdshrk/neuraldrummer?color=pink&style=flat-square)](https://github.com/bdshrk/neuraldrummer/graphs/contributors)|[![](https://flat.badgen.net/github/license/bdshrk/neuraldrummer?label=)](https://github.com/bdshrk/neuraldrummer/blob/main/LICENSEE)|[![](https://img.shields.io/github/last-commit/bdshrk/neuraldrummer?style=flat-square&label=)](https://github.com/bdshrk/neuraldrummer/graphs/code-frequency)|
|[Neural-Networks-for-Drum-Music-Generation](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation#readme)|Generating realistic drum music using LSTM neural networks trained on rock-style MIDI drum performances|[![](https://img.shields.io/github/languages/top/pareshraut/Neural-Networks-for-Drum-Music-Generation?color=pink&style=flat-square)](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation/graphs/contributors)|[![](https://flat.badgen.net/github/license/pareshraut/Neural-Networks-for-Drum-Music-Generation?label=)](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/pareshraut/Neural-Networks-for-Drum-Music-Generation?style=flat-square&label=)](https://github.com/pareshraut/Neural-Networks-for-Drum-Music-Generation/graphs/code-frequency)|
|[Automatic drums transcription using neural networks](https://github.com/pagrumiaux/drums_transcription#readme)|Internship's code for automatic drums transcription with neural networks|[![](https://img.shields.io/github/languages/top/pagrumiaux/drums_transcription?color=pink&style=flat-square)](https://github.com/pagrumiaux/drums_transcription/graphs/contributors)|[![](https://flat.badgen.net/github/license/pagrumiaux/drums_transcription?label=)](https://github.com/pagrumiaux/drums_transcription/issues/1)|[![](https://img.shields.io/github/last-commit/pagrumiaux/drums_transcription?style=flat-square&label=)](https://github.com/pagrumiaux/drums_transcription/graphs/code-frequency)|
|[Drum Transcription algorithm](https://github.com/juanangd/drums_transcription_algorithm#readme)|AI-based Drums Transcription Algorithm|[![](https://img.shields.io/github/languages/top/juanangd/drums_transcription_algorithm?color=pink&style=flat-square)](https://github.com/juanangd/drums_transcription_algorithm/graphs/contributors)|[![](https://flat.badgen.net/github/license/juanangd/drums_transcription_algorithm?label=)](https://github.com/juanangd/drums_transcription_algorithm/issues/2)|[![](https://img.shields.io/github/last-commit/juanangd/drums_transcription_algorithm?style=flat-square&label=)](https://github.com/juanangd/drums_transcription_algorithm/graphs/code-frequency)|


### Effects [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[OpenVINOâ„¢ AI Plugins for Audacity](https://github.com/intel/openvino-plugins-ai-audacity#readme)|A set of AI-enabled effects, generators, and analyzers for [AudacityÂ®](https://www.audacityteam.org/)|[![](https://img.shields.io/github/languages/top/intel/openvino-plugins-ai-audacity?color=pink&style=flat-square)](https://github.com/intel/openvino-plugins-ai-audacity/graphs/contributors)|[![](https://flat.badgen.net/github/license/intel/openvino-plugins-ai-audacity?label=)](https://github.com/intel/openvino-plugins-ai-audacity/blob/main/LICENSE.txt)|[![](https://img.shields.io/github/last-commit/intel/openvino-plugins-ai-audacity?style=flat-square&label=)](https://github.com/intel/openvino-plugins-ai-audacity/graphs/code-frequency)|
|[GRAND MATRON](https://github.com/nicholasbulka/grandMatronPlugin#readme)|An audio neural network plugin modeling a low pass filter built in [JUCE](https://juce.com/)|[![](https://img.shields.io/github/languages/top/nicholasbulka/grandMatronPlugin?color=pink&style=flat-square)](https://github.com/nicholasbulka/grandMatronPlugin/graphs/contributors)|[![](https://flat.badgen.net/github/license/nicholasbulka/grandMatronPlugin?label=)](https://github.com/nicholasbulka/grandMatronPlugin/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/nicholasbulka/grandMatronPlugin?style=flat-square&label=)](https://github.com/nicholasbulka/grandMatronPlugin/graphs/code-frequency)|
|[AI Compressor](https://github.com/jamesnapierstuart/19329-AI-Compressor#readme)|Implementing an Intelligent Dynamic Range Compressor Using Machine Learning Approaches|[![](https://img.shields.io/github/languages/top/jamesnapierstuart/19329-AI-Compressor?color=pink&style=flat-square)](https://github.com/jamesnapierstuart/19329-AI-Compressor/graphs/contributors)|[![](https://flat.badgen.net/github/license/jamesnapierstuart/19329-AI-Compressor?label=)](https://github.com/jamesnapierstuart/19329-AI-Compressor/issues/1)|[![](https://img.shields.io/github/last-commit/jamesnapierstuart/19329-AI-Compressor?style=flat-square&label=)](https://github.com/jamesnapierstuart/19329-AI-Compressor/graphs/code-frequency)|

### MIDI [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[AI improviser plugin](https://github.com/yeeking/ai-improviser-plugin#readme)|A VST plugin that learns from an incoming MIDI stream and improvises with what it has learnt|[![](https://img.shields.io/github/languages/top/yeeking/ai-improviser-plugin?color=pink&style=flat-square)](https://github.com/yeeking/ai-improviser-plugin/graphs/contributors)|[![](https://flat.badgen.net/github/license/yeeking/ai-improviser-plugin?label=)](https://github.com/yeeking/ai-improviser-plugin/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/yeeking/ai-improviser-plugin?style=flat-square&label=)](https://github.com/yeeking/ai-improviser-plugin/graphs/code-frequency)|
|[NeuralNote](https://github.com/DamRsn/NeuralNote#readme)|Audio Plugin for Audio to MIDI transcription using deep learning|[![](https://img.shields.io/github/languages/top/DamRsn/NeuralNote?color=pink&style=flat-square)](https://github.com/DamRsn/NeuralNote/graphs/contributors)|[![](https://flat.badgen.net/github/license/DamRsn/NeuralNote?label=)](https://github.com/DamRsn/NeuralNote/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/DamRsn/NeuralNote?style=flat-square&label=)](https://github.com/DamRsn/NeuralNote/graphs/code-frequency)|
|[RoboDrummer](https://github.com/lincolt/RoboDrummer#readme)|Midi drums generator based on deep neural network|[![](https://img.shields.io/github/languages/top/lincolt/RoboDrummer?color=pink&style=flat-square)](https://github.com/lincolt/RoboDrummer/graphs/contributors)|[![](https://flat.badgen.net/github/license/lincolt/RoboDrummer?label=)](https://github.com/lincolt/RoboDrummer/issues/1)|[![](https://img.shields.io/github/last-commit/lincolt/RoboDrummer?style=flat-square&label=)](https://github.com/lincolt/RoboDrummer/graphs/code-frequency)|
|[AI-Music-Composer](https://github.com/DamiPayne/AI-Music-Composer#readme)|A project that trains a LSTM recurrent neural network over a data-set of MIDI files.|[![](https://img.shields.io/github/languages/top/DamiPayne/AI-Music-Composer?color=pink&style=flat-square)](https://github.com/DamiPayne/AI-Music-Composer/graphs/contributors)|[![](https://flat.badgen.net/github/license/DamiPayne/AI-Music-Composer?label=)](https://github.com/DamiPayne/AI-Music-Composer/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/DamiPayne/AI-Music-Composer?style=flat-square&label=)](https://github.com/DamiPayne/AI-Music-Composer/graphs/code-frequency)|
|[Notochord](https://github.com/Intelligent-Instruments-Lab/notochord#readme)|A real-time neural network model for MIDI performances|[![](https://img.shields.io/github/languages/top/Intelligent-Instruments-Lab/notochord?color=pink&style=flat-square)](https://github.com/Intelligent-Instruments-Lab/notochord/graphs/contributors)|[![](https://flat.badgen.net/github/license/Intelligent-Instruments-Lab/notochord?label=)](https://github.com/Intelligent-Instruments-Lab/notochord/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Intelligent-Instruments-Lab/notochord?style=flat-square&label=)](https://github.com/Intelligent-Instruments-Lab/notochord/graphs/code-frequency)|
|[Midi-AI-Melody-Generator](https://github.com/vanstorm9/Midi-AI-Melody-Generator#readme)|Uses LSTM neural networks to compose new and original melodies by feeding it midi files|[![](https://img.shields.io/github/languages/top/vanstorm9/Midi-AI-Melody-Generator?color=pink&style=flat-square)](https://github.com/vanstorm9/Midi-AI-Melody-Generator/graphs/contributors)|[![](https://flat.badgen.net/github/license/vanstorm9/Midi-AI-Melody-Generator?label=)](https://github.com/vanstorm9/Midi-AI-Melody-Generator/issues/5)|[![](https://img.shields.io/github/last-commit/vanstorm9/Midi-AI-Melody-Generator?style=flat-square&label=)](https://github.com/vanstorm9/Midi-AI-Melody-Generator/graphs/code-frequency)|


### Mixing/Mastering [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[MasterIA](https://github.com/Esgr0bar/MasterIA#readme)|This project aims to develop an AI that can perform automated audio mixing and mastering based on user-defined preferences or by analyzing reference tracks|[![](https://img.shields.io/github/languages/top/Esgr0bar/MasterIA?color=pink&style=flat-square)](https://github.com/Esgr0bar/MasterIA/graphs/contributors)|[![](https://flat.badgen.net/github/license/Esgr0bar/MasterIA?label=)](https://github.com/Esgr0bar/MasterIA/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Esgr0bar/MasterIA?style=flat-square&label=)](https://github.com/Esgr0bar/MasterIA/graphs/code-frequency)|
|[MachineLearningDrumGainMixing](https://github.com/djmoffat/MachineLearningDrumGainMixing#readme)|Supplementary website with audio examples for [A Machine Learning Approach to Multitrack Gain Mixing of Drums](https://arxiv.org/abs/1810.06603v2) paper|[![](https://img.shields.io/github/languages/top/djmoffat/MachineLearningDrumGainMixing?color=pink&style=flat-square)](https://github.com/djmoffat/MachineLearningDrumGainMixing/graphs/contributors)|[![](https://flat.badgen.net/github/license/djmoffat/MachineLearningDrumGainMixing?label=)](https://github.com/djmoffat/MachineLearningDrumGainMixing/issues/1)|[![](https://img.shields.io/github/last-commit/djmoffat/MachineLearningDrumGainMixing?style=flat-square&label=)](https://github.com/djmoffat/MachineLearningDrumGainMixing/graphs/code-frequency)|
|[AudMIX](https://github.com/dssudake/AudMIX#readme)|A web-based system for processing Audio using Deep Learning|[![](https://img.shields.io/github/languages/top/dssudake/AudMIX?color=pink&style=flat-square)](https://github.com/dssudake/AudMIX/graphs/contributors)|[![](https://flat.badgen.net/github/license/dssudake/AudMIX?label=)](https://github.com/dssudake/AudMIX/issues/9)|[![](https://img.shields.io/github/last-commit/dssudake/AudMIX?style=flat-square&label=)](https://github.com/dssudake/AudMIX/graphs/code-frequency)|
|[ZoundZcope](https://github.com/MartinEnke/ZoundZcope_AI#readme)|A modular audio analysis and feedback tool that gives automated mixing and mastering suggestions based on extracted audio features|[![](https://img.shields.io/github/languages/top/MartinEnke/ZoundZcope_AI?color=pink&style=flat-square)](https://github.com/MartinEnke/ZoundZcope_AI/graphs/contributors)|[![](https://flat.badgen.net/github/license/MartinEnke/ZoundZcope_AI?label=)](https://github.com/MartinEnke/ZoundZcope_AI/issues/1)|[![](https://img.shields.io/github/last-commit/MartinEnke/ZoundZcope_AI?style=flat-square&label=)](https://github.com/MartinEnke/ZoundZcope_AI/graphs/code-frequency)|
|[Diff-MST](https://github.com/sai-soum/Diff-MST#readme)|Multitrack music mixing style transfer given a reference song using differentiable mixing console|[![](https://img.shields.io/github/languages/top/sai-soum/Diff-MST?color=pink&style=flat-square)](https://github.com/sai-soum/Diff-MST/graphs/contributors)|[<img src="https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png" height="20">](https://github.com/sai-soum/Diff-MST/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/sai-soum/Diff-MST?style=flat-square&label=)](https://github.com/sai-soum/Diff-MST/graphs/code-frequency)|
|[MixTastic](https://github.com/haranku16/mixtastic#readme)|Removes barriers to entry for new artists by providing AI-powered tools that assist with basic mixing and mastering tasks|[![](https://img.shields.io/github/languages/top/haranku16/mixtastic?color=pink&style=flat-square)](https://github.com/haranku16/mixtastic/graphs/contributors)|[![](https://flat.badgen.net/github/license/haranku16/mixtastic?label=)](https://github.com/haranku16/mixtastic/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/haranku16/mixtastic?style=flat-square&label=)](https://github.com/haranku16/mixtastic/graphs/code-frequency)|
|[deep-audio-mixer](https://github.com/apelykh/deep-audio-mixer#readme)|Deep Learning based system for audio mixing|[![](https://img.shields.io/github/languages/top/apelykh/deep-audio-mixer?color=pink&style=flat-square)](https://github.com/apelykh/deep-audio-mixer/graphs/contributors)|[![](https://flat.badgen.net/github/license/apelykh/deep-audio-mixer?label=)](https://github.com/apelykh/deep-audio-mixer/issues/1)|[![](https://img.shields.io/github/last-commit/apelykh/deep-audio-mixer?style=flat-square&label=)](https://github.com/apelykh/deep-audio-mixer/graphs/code-frequency)|
|[Evolving artificial neural networks for cross-adaptive audio effects](https://github.com/iver56/cross-adaptive-audio#readme)|Analysis of various features of the audio signal is used to adaptively control parameters of audio processing of the same signal|[![](https://img.shields.io/github/languages/top/iver56/cross-adaptive-audio?color=pink&style=flat-square)](https://github.com/iver56/cross-adaptive-audio/graphs/contributors)|[![](https://flat.badgen.net/github/license/iver56/cross-adaptive-audio?label=)](https://github.com/iver56/cross-adaptive-audio/blob/master/LICENCE)|[![](https://flat.badgen.net/static/status/Archived/624711?label=)](https://github.com/iver56/cross-adaptive-audio/graphs/code-frequency)|
|[DJtransGAN](https://github.com/ChenPaulYu/DJtransGAN#readme)|Code for "[Automatic DJ Transitions with Differentiable Audio Effects and Generative Adversarial Networks](https://arxiv.org/abs/2110.06525)" paper|[![](https://img.shields.io/github/languages/top/ChenPaulYu/DJtransGAN?color=pink&style=flat-square)](https://github.com/ChenPaulYu/DJtransGAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/ChenPaulYu/DJtransGAN?label=)](https://github.com/ChenPaulYu/DJtransGAN/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/ChenPaulYu/DJtransGAN?style=flat-square&label=)](https://github.com/ChenPaulYu/DJtransGAN/graphs/code-frequency)|
|[automix-toolkit](https://github.com/csteinmetz1/automix-toolkit#readme)|Models and datasets for training deep learning automatic mixing models|[![](https://img.shields.io/github/languages/top/csteinmetz1/automix-toolkit?color=pink&style=flat-square)](https://github.com/csteinmetz1/automix-toolkit/graphs/contributors)|[![](https://flat.badgen.net/github/license/csteinmetz1/automix-toolkit?label=)](https://github.com/csteinmetz1/automix-toolkit/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/csteinmetz1/automix-toolkit?style=flat-square&label=)](https://github.com/csteinmetz1/automix-toolkit/graphs/code-frequency)|
|[MixCNN](https://github.com/csteinmetz1/MixCNN#readme)|Mulitrack mix leveling with convolutional neural nets|[![](https://img.shields.io/github/languages/top/csteinmetz1/MixCNN?color=pink&style=flat-square)](https://github.com/csteinmetz1/MixCNN/graphs/contributors)|[![](https://flat.badgen.net/github/license/csteinmetz1/MixCNN?label=)](https://github.com/csteinmetz1/MixCNN/issues/1)|[![](https://img.shields.io/github/last-commit/csteinmetz1/MixCNN?style=flat-square&label=)](https://github.com/csteinmetz1/MixCNN/graphs/code-frequency)|



### Separating [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[deeper-wider-melody](https://github.com/drwangxian/deeper-wider-melody#readme)|Code for the "[Enhancing Vocal Melody Extraction with Multilevel Contexts](https://ieeexplore.ieee.org/document/10518111)" paper|[![](https://img.shields.io/github/languages/top/drwangxian/deeper-wider-melody?color=pink&style=flat-square)](https://github.com/drwangxian/deeper-wider-melody/graphs/contributors)|[![](https://flat.badgen.net/github/license/drwangxian/deeper-wider-melody?label=)](https://github.com/drwangxian/deeper-wider-melody/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/drwangxian/deeper-wider-melody?style=flat-square&label=)](https://github.com/drwangxian/deeper-wider-melody/graphs/code-frequency)|
|[audioss](https://github.com/victor23k/audioss#readme)|Audio source separation tool using a neural network|[![](https://img.shields.io/github/languages/top/victor23k/audioss?color=pink&style=flat-square)](https://github.com/victor23k/audioss/graphs/contributors)|[![](https://flat.badgen.net/github/license/victor23k/audioss?label=)](https://github.com/victor23k/audioss/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/victor23k/audioss?style=flat-square&label=)](https://github.com/victor23k/audioss/graphs/code-frequency)|
|[DeepConvSep](https://github.com/MTG/DeepConvSep#readme)|Deep Convolutional Neural Networks for Musical Source Separation|[![](https://img.shields.io/github/languages/top/MTG/DeepConvSep?color=pink&style=flat-square)](https://github.com/MTG/DeepConvSep/graphs/contributors)|[![](https://flat.badgen.net/github/license/MTG/DeepConvSep?label=)](https://github.com/MTG/DeepConvSep/blob/master/COPYING.txt)|[![](https://img.shields.io/github/last-commit/MTG/DeepConvSep?style=flat-square&label=)](https://github.com/MTG/DeepConvSep/graphs/code-frequency)|
|[Hyperbolic Audio Source Separation](https://github.com/merlresearch/hyper-unmix#readme)|Source code for training models and using the hyperbolic interface proposed in our ICASSP 2023 paper, "[Hyperbolic Audio Source Separation](https://arxiv.org/abs/2212.05008)" paper|[![](https://img.shields.io/github/languages/top/merlresearch/hyper-unmix?color=pink&style=flat-square)](https://github.com/merlresearch/hyper-unmix/graphs/contributors)|[![](https://flat.badgen.net/github/license/merlresearch/hyper-unmix?label=)](https://github.com/merlresearch/hyper-unmix/blob/main/LICENSE.md)|[![](https://img.shields.io/github/last-commit/merlresearch/hyper-unmix?style=flat-square&label=)](https://github.com/merlresearch/hyper-unmix/graphs/code-frequency)|
|[A Wavenet for Music Source Separation](https://github.com/francesclluis/source-separation-wavenet#readme)|A neural network for end-to-end music source separation|[![](https://img.shields.io/github/languages/top/francesclluis/source-separation-wavenet?color=pink&style=flat-square)](https://github.com/francesclluis/source-separation-wavenet/graphs/contributors)|[![](https://flat.badgen.net/github/license/francesclluis/source-separation-wavenet?label=)](https://github.com/francesclluis/source-separation-wavenet/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/francesclluis/source-separation-wavenet?style=flat-square&label=)](https://github.com/francesclluis/source-separation-wavenet/graphs/code-frequency)|
|[Audio Source Separation](https://github.com/Ankit123Mishra/audio-source-separation#readme)|Deep Neural Network model for Audio source separation|[![](https://img.shields.io/github/languages/top/Ankit123Mishra/audio-source-separation?color=pink&style=flat-square)](https://github.com/Ankit123Mishra/audio-source-separation/graphs/contributors)|[![](https://flat.badgen.net/github/license/Ankit123Mishra/audio-source-separation?label=)](https://github.com/Ankit123Mishra/audio-source-separation/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Ankit123Mishra/audio-source-separation?style=flat-square&label=)](https://github.com/Ankit123Mishra/audio-source-separation/graphs/code-frequency)|
|[AudioSep](https://github.com/Audio-AGI/AudioSep#readme)|Official implementation of "[Separate Anything You Describe](https://arxiv.org/abs/2308.05037)" paper|[![](https://img.shields.io/github/languages/top/Audio-AGI/AudioSep?color=pink&style=flat-square)](https://github.com/Audio-AGI/AudioSep/graphs/contributors)|[![](https://flat.badgen.net/github/license/Audio-AGI/AudioSep?label=)](https://github.com/Audio-AGI/AudioSep/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Audio-AGI/AudioSep?style=flat-square&label=)](https://github.com/Audio-AGI/AudioSep/graphs/code-frequency)|
|[Audio_Spleeter](https://github.com/Dev952/Audio_Spleeter#readme)|A deep learning model capable of identifying and isolating different instruments and vocal tracks from an audio mix|[![](https://img.shields.io/github/languages/top/Dev952/Audio_Spleeter?color=pink&style=flat-square)](https://github.com/Dev952/Audio_Spleeter/graphs/contributors)|[![](https://flat.badgen.net/github/license/Dev952/Audio_Spleeter?label=)](https://github.com/Dev952/Audio_Spleeter/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/Dev952/Audio_Spleeter?style=flat-square&label=)](https://github.com/Dev952/Audio_Spleeter/graphs/code-frequency)|
|[audio-source-separation](https://github.com/SConsul/audio-source-separation#readme)|[PyTorch](https://pytorch.org/) code for "[Monoaural Audio Source Separation Using Deep Convolutional Neural Networks](https://pdfs.semanticscholar.org/fede/f8eedef76692d805a6a3380159a95b79b4de.pdf)" paper to separate instruments from music using a low-latency neural network|[![](https://img.shields.io/github/languages/top/SConsul/audio-source-separation?color=pink&style=flat-square)](https://github.com/SConsul/audio-source-separation/graphs/contributors)|[![](https://flat.badgen.net/github/license/SConsul/audio-source-separation?label=)](https://github.com/SConsul/audio-source-separation/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/SConsul/audio-source-separation?style=flat-square&label=)](https://github.com/SConsul/audio-source-separation/graphs/code-frequency)|
|[BS-RoFormer](https://github.com/lucidrains/BS-RoFormer#readme)|Implementation of [Band Split Roformer](https://arxiv.org/abs/2309.02612), SOTA Attention network for music source separation out of ByteDance AI Labs|[![](https://img.shields.io/github/languages/top/lucidrains/BS-RoFormer?color=pink&style=flat-square)](https://github.com/lucidrains/BS-RoFormer/graphs/contributors)|[![](https://flat.badgen.net/github/license/lucidrains/BS-RoFormer?label=)](https://github.com/lucidrains/BS-RoFormer/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/lucidrains/BS-RoFormer?style=flat-square&label=)](https://github.com/lucidrains/BS-RoFormer/graphs/code-frequency)|
|[Continual Music Source Separation](https://github.com/pedrocg42/continual-music-source-separation#readme)|Repository dedicated to train models for the task of music source separation in the context of continual learning|[![](https://img.shields.io/github/languages/top/pedrocg42/continual-music-source-separation?color=pink&style=flat-square)](https://github.com/pedrocg42/continual-music-source-separation/graphs/contributors)|[![](https://flat.badgen.net/github/license/pedrocg42/continual-music-source-separation?label=)](https://github.com/pedrocg42/continual-music-source-separation/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/pedrocg42/continual-music-source-separation?style=flat-square&label=)](https://github.com/pedrocg42/continual-music-source-separation/graphs/code-frequency)|
|[GAN_SASS_TF](https://github.com/ahmedassal/GAN_SASS_TF#readme)|[TensorFlow](https://www.tensorflow.org/) implementation of "[GAN Single Audio Source Separation](https://hajim.rochester.edu/ece/sites/zduan/teaching/ece472/projects/2019/GANforAudioSourceSeparation.pdf)" paper|[![](https://img.shields.io/github/languages/top/ahmedassal/GAN_SASS_TF?color=pink&style=flat-square)](https://github.com/ahmedassal/GAN_SASS_TF/graphs/contributors)|[![](https://flat.badgen.net/github/license/ahmedassal/GAN_SASS_TF?label=)](https://github.com/ahmedassal/GAN_SASS_TF/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/ahmedassal/GAN_SASS_TF?style=flat-square&label=)](https://github.com/ahmedassal/GAN_SASS_TF/graphs/code-frequency)|
|[Music Source Separation Universal Training Code](https://github.com/ZFTurbo/Music-Source-Separation-Training#readme)|Repository for training models for music source separation|[![](https://img.shields.io/github/languages/top/ZFTurbo/Music-Source-Separation-Training?color=pink&style=flat-square)](https://github.com/ZFTurbo/Music-Source-Separation-Training/graphs/contributors)|[![](https://flat.badgen.net/github/license/ZFTurbo/Music-Source-Separation-Training?label=)](https://github.com/ZFTurbo/Music-Source-Separation-Training/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/ZFTurbo/Music-Source-Separation-Training?style=flat-square&label=)](https://github.com/ZFTurbo/Music-Source-Separation-Training/graphs/code-frequency)|
|[Open-Unmix for PyTorch](https://github.com/sigsep/open-unmix-pytorch#readme)|[PyTorch](https://pytorch.org/) (1.8+) implementation of Open-Unmix, a deep neural network reference implementation for music source separation|[![](https://img.shields.io/github/languages/top/sigsep/open-unmix-pytorch?color=pink&style=flat-square)](https://github.com/sigsep/open-unmix-pytorch/graphs/contributors)|[![](https://flat.badgen.net/github/license/sigsep/open-unmix-pytorch?label=)](https://github.com/sigsep/open-unmix-pytorch/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/sigsep/open-unmix-pytorch?style=flat-square&label=)](https://github.com/sigsep/open-unmix-pytorch/graphs/code-frequency)|
|[Spleeter](https://github.com/deezer/spleeter#readme)|Deezer source separation library including pretrained models.|[![](https://img.shields.io/github/languages/top/deezer/spleeter?color=pink&style=flat-square)](https://github.com/deezer/spleeter/graphs/contributors)|[![](https://flat.badgen.net/github/license/deezer/spleeter?label=)](https://github.com/deezer/spleeter/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/deezer/spleeter?style=flat-square&label=)](https://github.com/deezer/spleeter/graphs/code-frequency)|
|[Ultimate Vocal Remover GUI](https://github.com/Anjok07/ultimatevocalremovergui#readme)|A GUI for a Vocal Remover that uses Deep Neural Networks|[![](https://img.shields.io/github/languages/top/Anjok07/ultimatevocalremovergui?color=pink&style=flat-square)](https://github.com/Anjok07/ultimatevocalremovergui/graphs/contributors)|[![](https://flat.badgen.net/github/license/Anjok07/ultimatevocalremovergui?label=)](https://github.com/Anjok07/ultimatevocalremovergui/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/Anjok07/ultimatevocalremovergui?style=flat-square&label=)](https://github.com/Anjok07/ultimatevocalremovergui/graphs/code-frequency)|
|[BandIt: Cinematic Audio Source Separation](https://github.com/karnwatcharasupat/bandit#readme)|Code for "[A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation](https://arxiv.org/abs/2309.02539)" paper|[![](https://img.shields.io/github/languages/top/karnwatcharasupat/bandit?color=pink&style=flat-square)](https://github.com/karnwatcharasupat/bandit/graphs/contributors)|[![](https://flat.badgen.net/github/license/karnwatcharasupat/bandit?label=)](https://github.com/kwatcharasupat/bandit/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/karnwatcharasupat/bandit?style=flat-square&label=)](https://github.com/karnwatcharasupat/bandit/graphs/code-frequency)|
|[CASS](https://github.com/ongyongzheng/cass#readme)|CASS is a source separation model unifying AE and GAN frameworks|[![](https://img.shields.io/github/languages/top/ongyongzheng/cass?color=pink&style=flat-square)](https://github.com/ongyongzheng/cass/graphs/contributors)|[![](https://flat.badgen.net/github/license/ongyongzheng/cass?label=)](https://github.com/ongyongzheng/cass/issues/1)|[![](https://img.shields.io/github/last-commit/ongyongzheng/cass?style=flat-square&label=)](https://github.com/ongyongzheng/cass/graphs/code-frequency)|
|[Generative sourceseparation with GANs](https://github.com/ycemsubakan/sourceseparation_misc#readme)|Code for [Generative Adversarial Source Separation](https://arxiv.org/abs/1710.10779) paper|[![](https://img.shields.io/github/languages/top/ycemsubakan/sourceseparation_misc?color=pink&style=flat-square)](https://github.com/ycemsubakan/sourceseparation_misc/graphs/contributors)|[![](https://flat.badgen.net/github/license/ycemsubakan/sourceseparation_misc?label=)](https://github.com/ycemsubakan/sourceseparation_misc/issues/5)|[![](https://img.shields.io/github/last-commit/ycemsubakan/sourceseparation_misc?style=flat-square&label=)](https://github.com/ycemsubakan/sourceseparation_misc/graphs/code-frequency)|
|[Unmixer](https://github.com/lehenbauer/unmixer#readme)|A GUI frontend for [LALAL.AI](https://lalal.ai/)'s AI-powered stem-splitting technology|[![](https://img.shields.io/github/languages/top/lehenbauer/unmixer?color=pink&style=flat-square)](https://github.com/lehenbauer/unmixer/graphs/contributors)|[![](https://flat.badgen.net/github/license/lehenbauer/unmixer?label=)](https://github.com/lehenbauer/unmixer/issues/1)|[![](https://img.shields.io/github/last-commit/lehenbauer/unmixer?style=flat-square&label=)](https://github.com/lehenbauer/unmixer/graphs/code-frequency)|


### Fingerprinting [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[neural-fingerprinting](https://github.com/chrispla/neural-fingerprinting#readme)|Neural audio fingerprinting model similar to "[Neural audio fingerprint for high-specific audio retrieval based on contrastive learning](https://arxiv.org/abs/2010.11910)" but with a more lightweight encoder, simpler nearest neighbor search, and implemented in [PyTorch](https://pytorch.org/)|[![](https://img.shields.io/github/languages/top/chrispla/neural-fingerprinting?color=pink&style=flat-square)](https://github.com/chrispla/neural-fingerprinting/graphs/contributors)|[![](https://flat.badgen.net/github/license/chrispla/neural-fingerprinting?label=)](https://github.com/chrispla/neural-fingerprinting/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/chrispla/neural-fingerprinting?style=flat-square&label=)](https://github.com/chrispla/neural-fingerprinting/graphs/code-frequency)|
|[neural-audio-fp](https://github.com/mimbres/neural-audio-fp#readme)|[Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrasive Learning](https://arxiv.org/abs/2010.11910)|[![](https://img.shields.io/github/languages/top/mimbres/neural-audio-fp?color=pink&style=flat-square)](https://github.com/mimbres/neural-audio-fp/graphs/contributors)|[![](https://flat.badgen.net/github/license/mimbres/neural-audio-fp?label=)](https://github.com/mimbres/neural-audio-fp/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/mimbres/neural-audio-fp?style=flat-square&label=)](https://github.com/mimbres/neural-audio-fp/graphs/code-frequency)|
|[FingerprintDNN](https://github.com/carlmoore256/FingerprintDNN#readme)|Fast pitch detection using a deep neural network trained on audio fingerprints|[![](https://img.shields.io/github/languages/top/carlmoore256/FingerprintDNN?color=pink&style=flat-square)](https://github.com/carlmoore256/FingerprintDNN/graphs/contributors)|[![](https://flat.badgen.net/github/license/carlmoore256/FingerprintDNN?label=)](https://github.com/carlmoore256/FingerprintDNN/issues/1)|[![](https://img.shields.io/github/last-commit/carlmoore256/FingerprintDNN?style=flat-square&label=)](https://github.com/carlmoore256/FingerprintDNN/graphs/code-frequency)|
|[pfann](https://github.com/stdio2016/pfann#readme)|Unofficial reproduction of "[Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrasive Learning](https://arxiv.org/abs/2010.11910)" paper|[![](https://img.shields.io/github/languages/top/stdio2016/pfann?color=pink&style=flat-square)](https://github.com/stdio2016/pfann/graphs/contributors)|[![](https://flat.badgen.net/github/license/stdio2016/pfann?label=)](https://github.com/stdio2016/pfann/issues/3)|[![](https://img.shields.io/github/last-commit/stdio2016/pfann?style=flat-square&label=)](https://github.com/stdio2016/pfann/graphs/code-frequency)|

### Watermarking [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[DNN-audio-watermarking](https://github.com/kosta-pmf/dnn-audio-watermarking#readme)|A robust DNN-based audio watermarking system|[![](https://img.shields.io/github/languages/top/kosta-pmf/dnn-audio-watermarking?color=pink&style=flat-square)](https://github.com/kosta-pmf/dnn-audio-watermarking/graphs/contributors)|[![](https://flat.badgen.net/github/license/kosta-pmf/dnn-audio-watermarking?label=)](https://github.com/kosta-pmf/dnn-audio-watermarking/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/kosta-pmf/dnn-audio-watermarking?style=flat-square&label=)](https://github.com/kosta-pmf/dnn-audio-watermarking/graphs/code-frequency)|
|[WavMark](https://github.com/wavmark/wavmark#readme)|AI-based Audio Watermarking Tool|[![](https://img.shields.io/github/languages/top/wavmark/wavmark?color=pink&style=flat-square)](https://github.com/wavmark/wavmark/graphs/contributors)|[![](https://flat.badgen.net/github/license/wavmark/wavmark?label=)](https://github.com/wavmark/wavmark/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/wavmark/wavmark?style=flat-square&label=)](https://github.com/wavmark/wavmark/graphs/code-frequency)|
|[IDEAW](https://github.com/PecholaL/IDEAW#readme)|Robust Neural Audio Watermarking with Invertible Dual-Embedding|[![](https://img.shields.io/github/languages/top/PecholaL/IDEAW?color=pink&style=flat-square)](https://github.com/PecholaL/IDEAW/graphs/contributors)|[![](https://flat.badgen.net/github/license/PecholaL/IDEAW?label=)](https://github.com/PecholaL/IDEAW/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/PecholaL/IDEAW?style=flat-square&label=)](https://github.com/PecholaL/IDEAW/graphs/code-frequency)|[![](https://img.shields.io/github/last-commit/9rg/AutomaticTranscription-viaDL/master?label=)](https://github.com/9rg/AutomaticTranscription-viaDL/graphs/code-frequency)|

### Codecs [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[NeuralCodecs](https://github.com/DillionLowry/NeuralCodecs#readme)|Neural Audio Codecs implemented in C#|[![](https://img.shields.io/github/languages/top/DillionLowry/NeuralCodecs?color=pink&style=flat-square)](https://github.com/DillionLowry/NeuralCodecs/graphs/contributors)|[![](https://flat.badgen.net/github/license/DillionLowry/NeuralCodecs?label=)](https://github.com/DillionLowry/NeuralCodecs/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/DillionLowry/NeuralCodecs?style=flat-square&label=)](https://github.com/DillionLowry/NeuralCodecs/graphs/code-frequency)|
|[hilcodec](https://github.com/aask1357/hilcodec#readme)|High fidelity, lightweight, end-to-end, streaming, convolution-based neural audio codec|[![](https://img.shields.io/github/languages/top/aask1357/hilcodec?color=pink&style=flat-square)](https://github.com/aask1357/hilcodec/graphs/contributors)|[![](https://flat.badgen.net/github/license/aask1357/hilcodec?label=)](https://github.com/aask1357/hilcodec/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/aask1357/hilcodec?style=flat-square&label=)](https://github.com/aask1357/hilcodec/graphs/code-frequency)|
|[AudioDec](https://github.com/facebookresearch/AudioDec#readme)|An Open-source Streaming High-fidelity Neural Audio Codec|[![](https://img.shields.io/github/languages/top/facebookresearch/AudioDec?color=pink&style=flat-square)](https://github.com/facebookresearch/AudioDec/graphs/contributors)|[<img src="https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc.png" height="20">](https://github.com/facebookresearch/AudioDec/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/facebookresearch/AudioDec?style=flat-square&label=)](https://github.com/facebookresearch/AudioDec/graphs/code-frequency)|
|[FlowDec](https://github.com/facebookresearch/FlowDec#readme)|An neural full-band audio codec for general audio sampled at 48 kHz with 7.5 kps or 4.5 kbps.|[![](https://img.shields.io/github/languages/top/facebookresearch/FlowDec?color=pink&style=flat-square)](https://github.com/facebookresearch/FlowDec/graphs/contributors)|[<img src="https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc.png" height="20">](https://github.com/facebookresearch/FlowDec/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/facebookresearch/FlowDec?style=flat-square&label=)](https://github.com/facebookresearch/FlowDec/graphs/code-frequency)|
|[AudioCodec-Hub](https://github.com/ga642381/AudioCodec-Hub#readme)|A Python library for encoding and decoding audio data, supporting various neural audio codec models|[![](https://img.shields.io/github/languages/top/ga642381/AudioCodec-Hub?color=pink&style=flat-square)](https://github.com/ga642381/AudioCodec-Hub/graphs/contributors)|[![](https://flat.badgen.net/github/license/ga642381/AudioCodec-Hub?label=)](https://github.com/ga642381/AudioCodec-Hub/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/ga642381/AudioCodec-Hub/main?style=flat-square&label=)](https://github.com/ga642381/AudioCodec-Hub/graphs/code-frequency)|
|[Descript Audio Codec](https://github.com/descriptinc/descript-audio-codec#readme)|A high fidelity general neural audio codec|[![](https://img.shields.io/github/languages/top/descriptinc/descript-audio-codec?color=pink&style=flat-square)](https://github.com/descriptinc/descript-audio-codec/graphs/contributors)|[![](https://flat.badgen.net/github/license/descriptinc/descript-audio-codec?label=)](https://github.com/descriptinc/descript-audio-codec/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/descriptinc/descript-audio-codec/main?style=flat-square&label=)](https://github.com/descriptinc/descript-audio-codec/graphs/code-frequency)|
|[EnCodec](https://github.com/facebookresearch/encodec#readme)|State-of-the-art deep learning based audio codec|[![](https://img.shields.io/github/languages/top/facebookresearch/encodec?color=pink&style=flat-square)](https://github.com/facebookresearch/encodec/graphs/contributors)|[![](https://flat.badgen.net/github/license/facebookresearch/encodec?label=)](https://github.com/facebookresearch/encodec/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/facebookresearch/encodec/main?style=flat-square&label=)](https://github.com/facebookresearch/encodec/graphs/code-frequency)|
|[encodec-pytorch](https://github.com/NoFish-528/encodec-pytorch#readme)|unofficial implementation of the [High Fidelity Neural Audio Compression](https://arxiv.org/pdf/2210.13438.pdf)|[![](https://img.shields.io/github/languages/top/NoFish-528/encodec-pytorch?color=pink&style=flat-square)](https://github.com/NoFish-528/encodec-pytorch/graphs/contributors)|[![](https://flat.badgen.net/github/license/NoFish-528/encodec-pytorch?label=)](https://github.com/ZhikangNiu/encodec-pytorch/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/NoFish-528/encodec-pytorch/main?style=flat-square&label=)](https://github.com/NoFish-528/encodec-pytorch/graphs/code-frequency)|
|[NDVQ](https://github.com/ZhikangNiu/NDVQ#readme)|Official repository for the "[NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization](https://arxiv.org/abs/2409.12717)" paper|[![](https://img.shields.io/github/languages/top/ZhikangNiu/NDVQ?color=pink&style=flat-square)](https://github.com/ZhikangNiu/NDVQ/graphs/contributors)|[![](https://flat.badgen.net/github/license/ZhikangNiu/NDVQ?label=)](https://github.com/ZhikangNiu/NDVQ/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/ZhikangNiu/NDVQ?style=flat-square&label=)](https://github.com/ZhikangNiu/NDVQ/graphs/code-frequency)|
|[NeuralAudio](https://github.com/mattminuti/NeuralAudio#readme)|Sound compression based on Growing Self-Organizing Maps|[![](https://img.shields.io/github/languages/top/mattminuti/NeuralAudio?color=pink&style=flat-square)](https://github.com/mattminuti/NeuralAudio/graphs/contributors)|[![](https://flat.badgen.net/github/license/mattminuti/NeuralAudio?label=)](https://github.com/mattminuti/NeuralAudio/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/mattminuti/NeuralAudio?style=flat-square&label=)](https://github.com/mattminuti/NeuralAudio/graphs/code-frequency)|
|[Siamese SIREN](https://github.com/lucala/siamese-siren#readme)|Audio Compression with Implicit Neural Representations|[![](https://img.shields.io/github/languages/top/lucala/siamese-siren?color=pink&style=flat-square)](https://github.com/lucala/siamese-siren/graphs/contributors)|[![](https://flat.badgen.net/github/license/lucala/siamese-siren?label=)](https://github.com/lucala/siamese-siren/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/lucala/siamese-siren/main?style=flat-square&label=)](https://github.com/lucala/siamese-siren/graphs/code-frequency)|
|[SoundStream](https://github.com/haydenshively/SoundStream#readme)|An end-to-end neural audio codec|[![](https://img.shields.io/github/languages/top/haydenshively/SoundStream?color=pink&style=flat-square)](https://github.com/haydenshively/SoundStream/graphs/contributors)|[![](https://flat.badgen.net/github/license/haydenshively/SoundStream?label=)](https://github.com/haydenshively/SoundStream/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/haydenshively/SoundStream/master?style=flat-square&label=)](https://github.com/haydenshively/SoundStream/graphs/code-frequency)|
|[audiolite](https://github.com/samarthagali/audiolite#readme)|A deep learning framework for audio compression and decompression|[![](https://img.shields.io/github/languages/top/samarthagali/audiolite?color=pink&style=flat-square)](https://github.com/samarthagali/audiolite/graphs/contributors)|[![](https://flat.badgen.net/github/license/samarthagali/audiolite?label=)](https://github.com/samarthagali/audiolite/issues/2)|[![](https://img.shields.io/github/last-commit/samarthagali/audiolite?style=flat-square&label=)](https://github.com/samarthagali/audiolite/graphs/code-frequency)|
|[CorticalCodingCodec](https://github.com/itu-lab/CorticalCodingCodec#readme)|A neural multimedia codec that have comparable performance to current state-of-the-art lossy audio codecs|[![](https://img.shields.io/github/languages/top/itu-lab/CorticalCodingCodec?color=pink&style=flat-square)](https://github.com/itu-lab/CorticalCodingCodec/graphs/contributors)|[![](https://flat.badgen.net/github/license/itu-lab/CorticalCodingCodec?label=)](https://github.com/itu-lab/CorticalCodingCodec/issues/1)|[![](https://img.shields.io/github/last-commit/itu-lab/CorticalCodingCodec/main?style=flat-square&label=)](https://github.com/itu-lab/CorticalCodingCodec/graphs/code-frequency)|



### Transcripting [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[audioFlux](https://github.com/libAudioFlux/audioFlux#readme)|A deep learning tool library for audio and music analysis, feature extraction|[![](https://img.shields.io/github/languages/top/libAudioFlux/audioFlux?color=pink&style=flat-square)](https://github.com/libAudioFlux/audioFlux/graphs/contributors)|[![](https://flat.badgen.net/github/license/libAudioFlux/audioFlux?label=)](https://github.com/libAudioFlux/audioFlux/blob/master/LICENSE.md)|[![](https://img.shields.io/github/last-commit/libAudioFlux/audioFlux?style=flat-square&label=)](https://github.com/libAudioFlux/audioFlux/graphs/code-frequency)|
|[Guitabify](https://github.com/thru-goes-hamilton/Guitabify#readme)|An AI application that transcribes guitar audio to tabs|[![](https://img.shields.io/github/languages/top/thru-goes-hamilton/Guitabify?color=pink&style=flat-square)](https://github.com/thru-goes-hamilton/Guitabify/graphs/contributors)|[![](https://flat.badgen.net/github/license/thru-goes-hamilton/Guitabify?label=)](https://github.com/thru-goes-hamilton/Guitabify/blob/master/LICENSE)|[![](https://img.shields.io/github/last-commit/thru-goes-hamilton/Guitabify?style=flat-square&label=)](https://github.com/thru-goes-hamilton/Guitabify/graphs/code-frequency)|
|[AutomaticTranscription-viaDL](https://github.com/9rg/AutomaticTranscription-viaDL#readme)|Deep learning Japanese instruments - flute and drum - automatic transcription|[![](https://img.shields.io/github/languages/top/9rg/AutomaticTranscription-viaDL?color=pink&style=flat-square)](https://github.com/9rg/AutomaticTranscription-viaDL/graphs/contributors)|[![](https://flat.badgen.net/github/license/9rg/AutomaticTranscription-viaDL?label=)](https://github.com/9rg/AutomaticTranscription-viaDL/issues/1)|[![](https://img.shields.io/github/last-commit/9rg/AutomaticTranscription-viaDL?style=flat-square&label=)](https://github.com/9rg/AutomaticTranscription-viaDL/graphs/code-frequency)|


### Misc [âŒ‚](#--)
|Repository|Short description|Language|License|Last commit|
|:-:|:-:|:-:|:-:|:-:|
|[NeuralAudio](https://github.com/mikeoliphant/NeuralAudio#readme)|C++ library for real-time processing of audio neural network models|[![](https://img.shields.io/github/languages/top/mikeoliphant/NeuralAudio?color=pink&style=flat-square)](https://github.com/mikeoliphant/NeuralAudio/graphs/contributors)|[![](https://flat.badgen.net/github/license/mikeoliphant/NeuralAudio?label=)](https://github.com/mikeoliphant/NeuralAudio/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/mikeoliphant/NeuralAudio?style=flat-square&label=)](https://github.com/mikeoliphant/NeuralAudio/graphs/code-frequency)|
|[Bleeding Removal in Music Signals](https://github.com/its-rajesh/Audio-Bleeding-Removal#readme)|Neural networks for removal of bleeding in music signals for the sequential application of Music Source Separation|[![](https://img.shields.io/github/languages/top/its-rajesh/Audio-Bleeding-Removal?color=pink&style=flat-square)](https://github.com/its-rajesh/Audio-Bleeding-Removal/graphs/contributors)|[![](https://flat.badgen.net/github/license/its-rajesh/Audio-Bleeding-Removal?label=)](https://github.com/its-rajesh/Audio-Bleeding-Removal/blob/main/LICENSE)|[![](https://img.shields.io/github/last-commit/its-rajesh/Audio-Bleeding-Removal?style=flat-square&label=)](https://github.com/its-rajesh/Audio-Bleeding-Removal/graphs/code-frequency)|
|[Musical-Accompaniment-GAN](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN#readme)|Deep Learning project to create a model for accompaniment of piano tracks with guitar, strings, bass and drums|[![](https://img.shields.io/github/languages/top/RaphRozenblum/Musical-Accompaniment-GAN?color=pink&style=flat-square)](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN/graphs/contributors)|[![](https://flat.badgen.net/github/license/RaphRozenblum/Musical-Accompaniment-GAN?label=)](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN/issues/1)|[![](https://img.shields.io/github/last-commit/RaphRozenblum/Musical-Accompaniment-GAN/master?style=flat-square&label=)](https://github.com/RaphRozenblum/Musical-Accompaniment-GAN/graphs/code-frequency)|
